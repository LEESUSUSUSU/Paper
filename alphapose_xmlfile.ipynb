{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/somoon0422/Paper/blob/main/alphapose_xmlfile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITXFPsQFu_8P"
      },
      "source": [
        "**I modified the Pillow installation part of the AlphaPose Colab example program.**\n",
        "\n",
        "**This is not thoth000's original program.**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buuh2yIqm_uG",
        "outputId": "a6c5b127-232b-433d-a416-74698f493c7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cxMU0dmlnCT"
      },
      "outputs": [],
      "source": [
        "! pip install pyyaml==5.2\n",
        "! pip install scipy\n",
        "! pip install numpy\n",
        "! pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import yaml, scipy, os\n",
        "print(yaml.__version__)\n",
        "print(scipy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VBhQTOSoWab"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/AlphaPose\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/MVIG-SJTU/AlphaPose.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkkNtO8qolbz"
      },
      "outputs": [],
      "source": [
        "!python -m pip install cython\n",
        "!sudo apt-get install libyaml-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-Gw3k4coyFD"
      },
      "outputs": [],
      "source": [
        "\n",
        "os.chdir('/content/AlphaPose')\n",
        "print(os.getcwd())\n",
        "! python setup.py build develop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9tg0Z2RznSv"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "! mkdir /content/AlphaPose/detector/yolo/data\n",
        "file_id = '1D47msNOOiJKvPOXlnpyzdKA3k6E97NTC'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('/content/AlphaPose/detector/yolo/data/yolov3-spp.weights')\n",
        "\n",
        "! mkdir /content/AlphaPose/detector/tracker/data\n",
        "file_id = '1nlnuYfGNuHWZztQHXwVZSL_FvfE551pA'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('/content/AlphaPose/detector/tracker/data/JDE-1088x608-uncertainty')\n",
        "\n",
        "file_id = '1kQhnMRURFiy7NsdS8EFL-8vtqEXOgECn'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('/content/AlphaPose/pretrained_models/fast_res50_256x192.pth')\n",
        "\n",
        "!wget -P ./detector/yolox/data/ https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.0/yolox_x.pth\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiJCOUjj-g-M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/AlphaPose')\n",
        "! ls\n",
        "! python3 scripts/demo_inference.py --cfg configs/coco/resnet/256x192_res50_lr1e-3_1x.yaml --checkpoint pretrained_models/fast_res50_256x192.pth --indir examples/demo/ --save_img\n",
        "# result json and rendered images are saved here:\n",
        "! ls examples/res/\n",
        "! ls examples/res/vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C17tdN7MrmyL"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from alphapose.utils.config import update_config\n",
        "from alphapose.models import builder\n",
        "from alphapose.utils.transforms import get_func_heatmap_to_coord\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# 1. XML 파일에서 프레임 레이블 추출\n",
        "def parse_xml_for_time_labels(xml_file, fps):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # 액션 레이블을 저장할 딕셔너리 초기화\n",
        "    frame_labels = {}\n",
        "\n",
        "    # XML 파일에서 시작 시간과 액션 정보 추출\n",
        "    for event in root.findall('.//event'):\n",
        "        action_name = event.find('eventname').text.strip() if event.find('eventname') is not None else None\n",
        "        start_time = event.find('starttime').text.strip() if event.find('starttime') is not None else None\n",
        "        duration_time = event.find('duration').text.strip() if event.find('duration') is not None else None\n",
        "\n",
        "        if action_name and start_time and duration_time:\n",
        "            try:\n",
        "                # 시작 시간과 지속 시간을 초 단위로 변환\n",
        "                start_parts = list(map(float, start_time.split(\":\")))\n",
        "                start_seconds = start_parts[0] * 3600 + start_parts[1] * 60 + start_parts[2]\n",
        "\n",
        "                duration_parts = list(map(float, duration_time.split(\":\")))\n",
        "                duration_seconds = duration_parts[0] * 3600 + duration_parts[1] * 60 + duration_parts[2]\n",
        "\n",
        "                # 액션이 발생하는 프레임 범위 계산\n",
        "                start_frame = int(start_seconds * fps)\n",
        "                end_frame = int((start_seconds + duration_seconds) * fps)\n",
        "\n",
        "                # 각 프레임에 대해 액션 레이블 할당\n",
        "                for frame in range(start_frame, end_frame + 1):\n",
        "                    if frame not in frame_labels:\n",
        "                        frame_labels[frame] = []\n",
        "                    frame_labels[frame].append(action_name)\n",
        "            except ValueError as e:\n",
        "                print(f\"Error parsing time for action '{action_name}': {e}\")\n",
        "\n",
        "    return frame_labels\n",
        "\n",
        "# 2. 동영상에서 프레임 추출 및 레이블 할당\n",
        "def extract_frames_with_labels(video_path, output_dir, fps, frame_labels):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    if video_fps == 0:\n",
        "        print(f\"Warning: Unable to read FPS from video {video_path}. Setting FPS to default value 30.\")\n",
        "        video_fps = 30\n",
        "\n",
        "    if fps > video_fps:\n",
        "        print(f\"Warning: Specified FPS ({fps}) is higher than video FPS ({video_fps}) for {video_path}. Adjusting FPS to video FPS.\")\n",
        "        fps = video_fps\n",
        "\n",
        "    if fps == 0:\n",
        "        print(f\"Error: FPS for frame extraction is set to zero. Skipping this file {video_path}.\")\n",
        "        cap.release()\n",
        "        return []\n",
        "\n",
        "    interval = int(video_fps / fps)\n",
        "\n",
        "    if interval <= 0:\n",
        "        print(f\"Error: Interval for frame extraction is zero or negative for video {video_path}. Skipping this file.\")\n",
        "        cap.release()\n",
        "        return []\n",
        "\n",
        "    frame_count = 0\n",
        "    extracted_count = 0\n",
        "    frame_data = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % interval == 0:\n",
        "            frame_filename = os.path.join(output_dir, f\"frame_{extracted_count:05d}.jpg\")\n",
        "            cv2.imwrite(frame_filename, frame)\n",
        "\n",
        "            # 레이블 할당\n",
        "            frame_label = frame_labels.get(frame_count, ['neutral'])\n",
        "            frame_data.append((frame_filename, frame_label))\n",
        "\n",
        "            extracted_count += 1\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Extracted {extracted_count} frames from {video_path} with labels.\")\n",
        "    return frame_data\n",
        "\n",
        "# 3. YOLO와 AlphaPose를 사용한 객체 탐지 및 키포인트 추출\n",
        "def detect_and_extract_keypoints(image_path, yolo_model, pose_model, cfg, device):\n",
        "    input_image = cv2.imread(image_path)\n",
        "    results = yolo_model.predict(input_image, save=False, classes=[0])\n",
        "    human_detections = [d for d in results[0].boxes.data.cpu().numpy() if int(d[-1]) == 0]\n",
        "\n",
        "    if not human_detections:\n",
        "        return [], [], []\n",
        "\n",
        "    inps = []\n",
        "    boxes = []\n",
        "    for detection in human_detections:\n",
        "        x1, y1, x2, y2 = map(int, detection[:4])\n",
        "        boxes.append([x1, y1, x2, y2])\n",
        "        inp = cv2.resize(input_image[y1:y2, x1:x2], (cfg.DATA_PRESET.IMAGE_SIZE[0], cfg.DATA_PRESET.IMAGE_SIZE[1]))\n",
        "        inps.append(inp)\n",
        "\n",
        "    inps = torch.stack([torch.from_numpy(np.array(inp)).permute(2, 0, 1).float() for inp in inps]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hm = pose_model(inps)\n",
        "\n",
        "    keypoints = []\n",
        "    for i, box in enumerate(boxes):\n",
        "        preds, maxvals = get_func_heatmap_to_coord(cfg)(hm[i], box)\n",
        "        keypoints_flatten = preds.flatten().tolist()\n",
        "        keypoints.append(keypoints_flatten)\n",
        "\n",
        "    return boxes, keypoints, ['person' for _ in boxes]\n",
        "\n",
        "# 4. 메인 함수: 전체 전처리 과정 수행 및 레이블 저장\n",
        "def main(root_dir, output_dir, csv_file_path, fps=30):\n",
        "    yolo_model = YOLO('yolov8n.pt')\n",
        "    cfg_file = '/content/AlphaPose/configs/coco/resnet/256x192_res50_lr1e-3_1x.yaml'\n",
        "    pretrained_model_path = '/content/drive/MyDrive/논문주제/Final_project/pretrained_models/fast_res50_256x192.pth'\n",
        "\n",
        "    cfg = update_config(cfg_file)\n",
        "    cfg['checkpoint'] = pretrained_model_path\n",
        "    cfg['vis'] = True\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    pose_model = builder.build_sppe(cfg.MODEL, preset_cfg=cfg.DATA_PRESET)\n",
        "    print(f'Loading pose model from {cfg[\"checkpoint\"]}')\n",
        "    pose_model.load_state_dict(torch.load(cfg[\"checkpoint\"], map_location=device))\n",
        "    pose_model = torch.nn.DataParallel(pose_model).to(device)\n",
        "    pose_model.eval()\n",
        "\n",
        "    all_keypoints = []  # 전체 키포인트 데이터를 누적하는 리스트\n",
        "\n",
        "    for subdir, dirs, files in os.walk(root_dir):\n",
        "        for folder in dirs:\n",
        "            folder_path = os.path.join(subdir, folder)\n",
        "            video_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.mp4', '.avi', '.mov'))]\n",
        "\n",
        "            for file in video_files:\n",
        "                video_path = os.path.join(folder_path, file)\n",
        "                xml_filename = os.path.splitext(file)[0] + '.xml'\n",
        "                xml_path = os.path.join(folder_path, xml_filename)\n",
        "\n",
        "                if not os.path.exists(xml_path):\n",
        "                    print(f'XML file not found for {video_path}, skipping...')\n",
        "                    continue\n",
        "\n",
        "                # XML 파일에서 레이블 추출\n",
        "                frame_labels = parse_xml_for_time_labels(xml_path, fps)\n",
        "\n",
        "                # 출력 디렉토리 설정\n",
        "                specific_output_dir = os.path.join(output_dir, os.path.relpath(folder_path, root_dir))\n",
        "                frame_output_dir = os.path.join(specific_output_dir, os.path.splitext(file)[0] + '_frames')\n",
        "                os.makedirs(frame_output_dir, exist_ok=True)\n",
        "\n",
        "                # 프레임 추출 및 레이블 할당\n",
        "                frame_data = extract_frames_with_labels(video_path, frame_output_dir, fps, frame_labels)\n",
        "\n",
        "                # 키포인트 추출 및 누적\n",
        "                for frame_file, labels in frame_data:\n",
        "                    boxes, keypoints, _ = detect_and_extract_keypoints(frame_file, yolo_model, pose_model, cfg, device)\n",
        "\n",
        "                    frame_name = os.path.basename(frame_file)\n",
        "\n",
        "                    if len(keypoints) >= 2:\n",
        "                        keypoints_data = [frame_name, labels[0]] + keypoints[0] + [labels[1] if len(labels) > 1 else ''] + (keypoints[1] if len(keypoints) > 1 else [0] * 34)\n",
        "                    elif len(keypoints) == 1:\n",
        "                        keypoints_data = [frame_name, labels[0]] + keypoints[0] + [''] + [0] * 34\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                    all_keypoints.append(keypoints_data)\n",
        "\n",
        "    # CSV 파일에 최종 저장\n",
        "    if all_keypoints:\n",
        "        df_temp = pd.DataFrame(all_keypoints, columns=[\n",
        "            'frame_name', 'target_person_label', 'target_person_nose_x', 'target_person_nose_y',\n",
        "            'target_person_left_eye_x', 'target_person_left_eye_y', 'target_person_right_eye_x', 'target_person_right_eye_y',\n",
        "            'target_person_left_ear_x', 'target_person_left_ear_y', 'target_person_right_ear_x', 'target_person_right_ear_y',\n",
        "            'target_person_left_shoulder_x', 'target_person_left_shoulder_y', 'target_person_right_shoulder_x',\n",
        "            'target_person_right_shoulder_y', 'target_person_left_elbow_x', 'target_person_left_elbow_y', 'target_person_right_elbow_x',\n",
        "            'target_person_right_elbow_y', 'target_person_left_wrist_x', 'target_person_left_wrist_y', 'target_person_right_wrist_x',\n",
        "            'target_person_right_wrist_y', 'target_person_left_hip_x', 'target_person_left_hip_y', 'target_person_right_hip_x',\n",
        "            'target_person_right_hip_y', 'target_person_left_knee_x', 'target_person_left_knee_y', 'target_person_right_knee_x',\n",
        "            'target_person_right_knee_y', 'target_person_left_ankle_x', 'target_person_left_ankle_y', 'target_person_right_ankle_x',\n",
        "            'target_person_right_ankle_y', 'closest_person_label', 'closest_person_nose_x', 'closest_person_nose_y', 'closest_person_left_eye_x',\n",
        "            'closest_person_left_eye_y', 'closest_person_right_eye_x', 'closest_person_right_eye_y', 'closest_person_left_ear_x',\n",
        "            'closest_person_left_ear_y', 'closest_person_right_ear_x', 'closest_person_right_ear_y', 'closest_person_left_shoulder_x',\n",
        "            'closest_person_left_shoulder_y', 'closest_person_right_shoulder_x', 'closest_person_right_shoulder_y', 'closest_person_left_elbow_x',\n",
        "            'closest_person_left_elbow_y', 'closest_person_right_elbow_x', 'closest_person_right_elbow_y', 'closest_person_left_wrist_x',\n",
        "            'closest_person_left_wrist_y', 'closest_person_right_wrist_x', 'closest_person_right_wrist_y', 'closest_person_left_hip_x',\n",
        "            'closest_person_left_hip_y', 'closest_person_right_hip_x', 'closest_person_right_hip_y', 'closest_person_left_knee_x',\n",
        "            'closest_person_left_knee_y', 'closest_person_right_knee_x', 'closest_person_right_knee_y', 'closest_person_left_ankle_x',\n",
        "            'closest_person_left_ankle_y', 'closest_person_right_ankle_x', 'closest_person_right_ankle_y'\n",
        "        ])\n",
        "        df_temp.to_csv(csv_file_path, mode='a', header=not os.path.exists(csv_file_path), index=False)\n",
        "        print(f'Final progress saved at {csv_file_path}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    root_dir = '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/01.폭행(assult)'\n",
        "    output_dir = '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/01.폭행(assult)'\n",
        "    csv_file_path = '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/01.폭행(assult)/results.csv'\n",
        "    main(root_dir, output_dir, csv_file_path, fps=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K7RryumnqpJD",
        "outputId": "a0e149fa-108c-49eb-f049-8ba96053c947"
      },
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 3.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.1ms\n",
            "Speed: 3.1ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 8.0ms\n",
            "Speed: 3.4ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.8ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.4ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 3.1ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.5ms\n",
            "Speed: 2.8ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.4ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 3.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.4ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.5ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.8ms\n",
            "Speed: 2.8ms preprocess, 9.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.6ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.6ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 3.4ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.4ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 3.5ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.3ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.9ms\n",
            "Speed: 3.3ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.2ms\n",
            "Speed: 3.6ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.5ms\n",
            "Speed: 2.8ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.9ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.3ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 10.1ms\n",
            "Speed: 3.0ms preprocess, 10.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.6ms\n",
            "Speed: 3.0ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.4ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.0ms\n",
            "Speed: 3.1ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.3ms\n",
            "Speed: 2.9ms preprocess, 8.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.4ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 3.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.6ms\n",
            "Speed: 3.5ms preprocess, 9.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 3.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.7ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 3.2ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.1ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.3ms\n",
            "Speed: 3.7ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 3.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.8ms\n",
            "Speed: 3.2ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.2ms\n",
            "Speed: 3.0ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.8ms\n",
            "Speed: 2.9ms preprocess, 9.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.3ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.4ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.9ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 3.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 3.0ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.6ms\n",
            "Speed: 3.2ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 3.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.2ms\n",
            "Speed: 2.5ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.5ms\n",
            "Speed: 2.7ms preprocess, 8.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 6.8ms\n",
            "Speed: 2.9ms preprocess, 6.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.3ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 3.1ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 3.4ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 9.5ms\n",
            "Speed: 3.3ms preprocess, 9.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 10.1ms\n",
            "Speed: 2.9ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 3.0ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 3.4ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 9.1ms\n",
            "Speed: 2.7ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.0ms\n",
            "Speed: 3.2ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.5ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.5ms preprocess, 7.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.0ms\n",
            "Speed: 3.1ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 8.1ms\n",
            "Speed: 3.0ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.8ms\n",
            "Speed: 2.9ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 3.1ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 3.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 3.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.7ms\n",
            "Speed: 3.3ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.0ms\n",
            "Speed: 2.7ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 10.2ms\n",
            "Speed: 3.6ms preprocess, 10.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 4.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.6ms\n",
            "Speed: 3.0ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.4ms\n",
            "Speed: 3.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.8ms\n",
            "Speed: 3.0ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.6ms\n",
            "Speed: 2.8ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.5ms\n",
            "Speed: 3.1ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.0ms\n",
            "Speed: 3.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 3.3ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.5ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.7ms\n",
            "Speed: 2.9ms preprocess, 8.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.2ms\n",
            "Speed: 2.7ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.5ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.4ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.3ms preprocess, 7.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.2ms\n",
            "Speed: 3.2ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 3.1ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.9ms\n",
            "Speed: 3.5ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 3.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 9.3ms\n",
            "Speed: 3.1ms preprocess, 9.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 10.1ms\n",
            "Speed: 3.7ms preprocess, 10.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.5ms\n",
            "Speed: 2.9ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 3.9ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.9ms\n",
            "Speed: 3.3ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.1ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.4ms\n",
            "Speed: 2.9ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 10.0ms\n",
            "Speed: 3.5ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.4ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 3.3ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.3ms\n",
            "Speed: 3.2ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.5ms\n",
            "Speed: 2.8ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 3.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.0ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.4ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 3.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 2.7ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 10.1ms\n",
            "Speed: 3.1ms preprocess, 10.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.7ms\n",
            "Speed: 2.8ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.4ms\n",
            "Speed: 3.0ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.0ms\n",
            "Speed: 2.9ms preprocess, 9.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 3.3ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.4ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.4ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.9ms\n",
            "Speed: 3.6ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.5ms\n",
            "Speed: 3.1ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.5ms\n",
            "Speed: 3.7ms preprocess, 8.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.6ms\n",
            "Speed: 3.1ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.2ms\n",
            "Speed: 2.6ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.9ms\n",
            "Speed: 2.9ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.2ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.6ms\n",
            "Speed: 3.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.6ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.5ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 2.8ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 3.0ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 3.2ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 2.7ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.7ms\n",
            "Speed: 2.8ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 3.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.5ms\n",
            "Speed: 3.9ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 2.9ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 6.9ms\n",
            "Speed: 2.8ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.2ms\n",
            "Speed: 3.0ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.4ms\n",
            "Speed: 2.8ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 3.6ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 3.2ms preprocess, 8.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 2.9ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.5ms\n",
            "Speed: 2.9ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.4ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.3ms\n",
            "Speed: 2.5ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 3.2ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.0ms\n",
            "Speed: 3.3ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.2ms\n",
            "Speed: 2.9ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.2ms\n",
            "Speed: 2.9ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.2ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 3.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.5ms\n",
            "Speed: 3.5ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 3.0ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.1ms\n",
            "Speed: 3.0ms preprocess, 9.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 12.5ms\n",
            "Speed: 3.0ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 3.7ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 2.8ms preprocess, 8.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.9ms\n",
            "Speed: 3.2ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 2.9ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.9ms\n",
            "Speed: 2.8ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.9ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.6ms\n",
            "Speed: 3.6ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 3.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 3.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 3.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.2ms\n",
            "Speed: 3.0ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 9.7ms\n",
            "Speed: 3.5ms preprocess, 9.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 9.7ms\n",
            "Speed: 3.6ms preprocess, 9.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.6ms\n",
            "Speed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 10.0ms\n",
            "Speed: 2.7ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 8.1ms\n",
            "Speed: 3.4ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.7ms\n",
            "Speed: 2.7ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.1ms\n",
            "Speed: 3.1ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.5ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 3.6ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 3.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.6ms\n",
            "Speed: 3.2ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.1ms\n",
            "Speed: 3.0ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.3ms\n",
            "Speed: 3.3ms preprocess, 8.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.1ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.4ms\n",
            "Speed: 2.8ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.0ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.6ms\n",
            "Speed: 3.1ms preprocess, 9.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 8.1ms\n",
            "Speed: 3.1ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 3.1ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.0ms\n",
            "Speed: 3.2ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 3.3ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.5ms\n",
            "Speed: 3.1ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.9ms\n",
            "Speed: 3.2ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.3ms\n",
            "Speed: 3.2ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.0ms\n",
            "Speed: 3.1ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 3.4ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 3.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.5ms\n",
            "Speed: 3.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.0ms\n",
            "Speed: 3.5ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.0ms\n",
            "Speed: 2.5ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.8ms\n",
            "Speed: 3.0ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.0ms\n",
            "Speed: 3.2ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.1ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 3.4ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 3.0ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.9ms\n",
            "Speed: 3.6ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 3.0ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.6ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 3.4ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.9ms\n",
            "Speed: 2.3ms preprocess, 6.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.6ms\n",
            "Speed: 3.4ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.4ms\n",
            "Speed: 3.5ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 3.3ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 3.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.4ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.0ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.6ms\n",
            "Speed: 2.9ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.8ms\n",
            "Speed: 3.7ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.9ms\n",
            "Speed: 2.9ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.5ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.2ms\n",
            "Speed: 3.0ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 3.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.8ms\n",
            "Speed: 3.5ms preprocess, 9.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.6ms\n",
            "Speed: 3.2ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 6.9ms\n",
            "Speed: 2.4ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.9ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 3.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.0ms\n",
            "Speed: 2.4ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 8.5ms\n",
            "Speed: 3.1ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 3.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 3.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 8.1ms\n",
            "Speed: 2.7ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.6ms\n",
            "Speed: 3.4ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 10.2ms\n",
            "Speed: 3.5ms preprocess, 10.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 3.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.5ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 3.9ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.9ms preprocess, 7.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.3ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.9ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 6.9ms\n",
            "Speed: 2.9ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.6ms\n",
            "Speed: 3.5ms preprocess, 8.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 3.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.2ms\n",
            "Speed: 2.7ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.6ms\n",
            "Speed: 3.4ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 3.0ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 10.0ms\n",
            "Speed: 3.0ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.1ms\n",
            "Speed: 3.0ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 3.5ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 3.2ms preprocess, 8.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.6ms\n",
            "Speed: 2.7ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.7ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.4ms\n",
            "Speed: 3.4ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.9ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.3ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.4ms\n",
            "Speed: 2.8ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.4ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.1ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.1ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.4ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 3.1ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 3.1ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 2.8ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.0ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.1ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.5ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.6ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.2ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.8ms\n",
            "Speed: 3.1ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.7ms\n",
            "Speed: 2.9ms preprocess, 8.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 3.6ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 3.3ms preprocess, 8.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.2ms\n",
            "Speed: 3.6ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 10.3ms\n",
            "Speed: 2.9ms preprocess, 10.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.9ms\n",
            "Speed: 3.3ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.4ms\n",
            "Speed: 3.0ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.9ms\n",
            "Speed: 3.3ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.6ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 3.0ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.9ms\n",
            "Speed: 2.9ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 3.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 10.0ms\n",
            "Speed: 3.6ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.4ms\n",
            "Speed: 3.4ms preprocess, 9.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 3.1ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.4ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.8ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.8ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.5ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.2ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.8ms preprocess, 7.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.7ms\n",
            "Speed: 2.8ms preprocess, 8.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.1ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.0ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.2ms\n",
            "Speed: 3.1ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.2ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.7ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.0ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.9ms\n",
            "Speed: 3.2ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.3ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.1ms\n",
            "Speed: 3.2ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 10.0ms\n",
            "Speed: 3.4ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.9ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 3.7ms preprocess, 7.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.0ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.7ms preprocess, 7.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.2ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.0ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.1ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.7ms\n",
            "Speed: 2.9ms preprocess, 8.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 2.8ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.5ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 3.1ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.9ms\n",
            "Speed: 3.8ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 10.0ms\n",
            "Speed: 2.9ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 3.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.9ms\n",
            "Speed: 3.2ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 3.6ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 3.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 3.3ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 3.1ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.3ms\n",
            "Speed: 3.0ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 3.0ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.8ms\n",
            "Speed: 3.1ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.2ms\n",
            "Speed: 3.7ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.8ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 4.0ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.7ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.9ms\n",
            "Speed: 3.2ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 2.7ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.0ms preprocess, 8.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 3.3ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 3.5ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.6ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.9ms\n",
            "Speed: 3.1ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.0ms\n",
            "Speed: 2.7ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.4ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.9ms\n",
            "Speed: 2.8ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 2.9ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 3.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.9ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.0ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.9ms\n",
            "Speed: 3.3ms preprocess, 9.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 11.8ms\n",
            "Speed: 4.1ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.1ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.6ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 3.1ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.8ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.1ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 3.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 3.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 3.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.0ms\n",
            "Speed: 3.0ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 3.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 3.4ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 3.4ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 3.0ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.6ms\n",
            "Speed: 3.0ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 3.3ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.3ms\n",
            "Speed: 3.6ms preprocess, 8.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 3.5ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 3.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 3.0ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 2.7ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.0ms\n",
            "Speed: 2.8ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 3.1ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.4ms preprocess, 7.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 3.7ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.1ms\n",
            "Speed: 2.7ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.5ms\n",
            "Speed: 2.9ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 3.8ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.1ms\n",
            "Speed: 3.1ms preprocess, 9.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 3.6ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 3.5ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.4ms\n",
            "Speed: 3.2ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.6ms\n",
            "Speed: 2.7ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 3.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 3.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 3.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 3.4ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.2ms\n",
            "Speed: 3.6ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.0ms\n",
            "Speed: 2.8ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.3ms\n",
            "Speed: 2.9ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.3ms\n",
            "Speed: 2.9ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.1ms\n",
            "Speed: 3.5ms preprocess, 9.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.2ms\n",
            "Speed: 3.0ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.5ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.5ms\n",
            "Speed: 2.7ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 3.4ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.3ms\n",
            "Speed: 3.5ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.2ms\n",
            "Speed: 2.7ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.3ms\n",
            "Speed: 3.8ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 2.9ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 3.7ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 3.8ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 3.3ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 2.8ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.6ms\n",
            "Speed: 3.2ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.3ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.6ms\n",
            "Speed: 2.7ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.2ms\n",
            "Speed: 3.0ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.1ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.9ms\n",
            "Speed: 3.1ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.4ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.1ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.9ms\n",
            "Speed: 3.1ms preprocess, 9.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.4ms\n",
            "Speed: 3.0ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.2ms\n",
            "Speed: 3.0ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.0ms\n",
            "Speed: 3.0ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.1ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.5ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.5ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.4ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.9ms\n",
            "Speed: 3.4ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.3ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.8ms\n",
            "Speed: 3.1ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.6ms\n",
            "Speed: 3.2ms preprocess, 9.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 3.2ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.5ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.3ms\n",
            "Speed: 3.1ms preprocess, 9.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.2ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.3ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.0ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.9ms\n",
            "Speed: 3.1ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 10.1ms\n",
            "Speed: 2.9ms preprocess, 10.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 2.9ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.3ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.0ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.6ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.0ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.3ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.9ms\n",
            "Speed: 3.5ms preprocess, 9.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.8ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 3.3ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.1ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.5ms\n",
            "Speed: 3.9ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 10.5ms\n",
            "Speed: 3.6ms preprocess, 10.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.4ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.0ms\n",
            "Speed: 3.0ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.0ms\n",
            "Speed: 3.0ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.6ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.6ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 3.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.4ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.2ms\n",
            "Speed: 3.1ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.5ms\n",
            "Speed: 3.6ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 3.3ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.4ms\n",
            "Speed: 2.9ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1d5f6d768d00>\u001b[0m in \u001b[0;36m<cell line: 217>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/01.폭행(assult)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mcsv_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/01.폭행(assult)/results.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-1d5f6d768d00>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(root_dir, output_dir, csv_file_path, fps)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;31m# 키포인트 추출 및 누적\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mframe_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframe_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_and_extract_keypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpose_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0mframe_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-1d5f6d768d00>\u001b[0m in \u001b[0;36mdetect_and_extract_keypoints\u001b[0;34m(image_path, yolo_model, pose_model, cfg, device)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# 3. YOLO와 AlphaPose를 사용한 객체 탐지 및 키포인트 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetect_and_extract_keypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpose_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0minput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mhuman_detections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvxySEI5AzC9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R19psejwxn_w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "d76dab07-e573-4f9a-feaa-13fd017a3760"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        frame_name target_person_label  target_person_nose_x  \\\n",
              "0  frame_00000.jpg             neutral           2129.703125   \n",
              "1  frame_00001.jpg             neutral           2214.054688   \n",
              "2  frame_00002.jpg             neutral           2140.757812   \n",
              "3  frame_00003.jpg             neutral           2213.507812   \n",
              "4  frame_00004.jpg             neutral           2150.843750   \n",
              "5  frame_00005.jpg             neutral           2146.253906   \n",
              "6  frame_00006.jpg             neutral           2153.328125   \n",
              "7  frame_00007.jpg             neutral           2151.421875   \n",
              "8  frame_00008.jpg             neutral           2144.757812   \n",
              "9  frame_00009.jpg             neutral           2150.781250   \n",
              "\n",
              "   target_person_nose_y  target_person_left_eye_x  target_person_left_eye_y  \\\n",
              "0            821.953125               2124.753906                831.144531   \n",
              "1            838.570312               2125.054688                838.570312   \n",
              "2            833.632812               2089.226562                833.632812   \n",
              "3            834.867188               2093.273438                834.867188   \n",
              "4            824.593750               2123.117188                833.835938   \n",
              "5            833.441406               2143.238281                833.441406   \n",
              "6            821.953125               2125.753906                831.144531   \n",
              "7            822.796875               2122.933594                832.292969   \n",
              "8            832.632812               2093.226562                832.632812   \n",
              "9            828.531250               2123.664062                837.570312   \n",
              "\n",
              "   target_person_right_eye_x  target_person_right_eye_y  \\\n",
              "0                2127.582031                 831.144531   \n",
              "1                2126.445312                 838.570312   \n",
              "2                2116.507812                 833.632812   \n",
              "3                2096.242188                 834.867188   \n",
              "4                2150.843750                 824.593750   \n",
              "5                2148.515625                 823.640625   \n",
              "6                2153.328125                 821.953125   \n",
              "7                2151.421875                 822.796875   \n",
              "8                2147.031250                 822.781250   \n",
              "9                2126.445312                 837.570312   \n",
              "\n",
              "   target_person_left_ear_x  target_person_left_ear_y  ...  \\\n",
              "0               2126.167969                831.144531  ...   \n",
              "1               2125.054688                838.570312  ...   \n",
              "2               2139.242188                833.632812  ...   \n",
              "3               2142.257812                834.867188  ...   \n",
              "4               2147.289062                832.414062  ...   \n",
              "5               2144.746094                831.933594  ...   \n",
              "6               2149.792969                829.730469  ...   \n",
              "7               2147.769531                830.832031  ...   \n",
              "8               2143.242188                831.117188  ...   \n",
              "9               2147.304688                837.570312  ...   \n",
              "\n",
              "   closest_person_right_hip_x  closest_person_right_hip_y  \\\n",
              "0                    0.000000                    0.000000   \n",
              "1                    0.000000                    0.000000   \n",
              "2                 2380.789062                  705.898438   \n",
              "3                 2374.335938                  711.476562   \n",
              "4                    0.000000                    0.000000   \n",
              "5                    0.000000                    0.000000   \n",
              "6                    0.000000                    0.000000   \n",
              "7                 2293.492188                  784.554688   \n",
              "8                    0.000000                    0.000000   \n",
              "9                 2363.269531                  751.074219   \n",
              "\n",
              "   closest_person_left_knee_x  closest_person_left_knee_y  \\\n",
              "0                    0.000000                    0.000000   \n",
              "1                    0.000000                    0.000000   \n",
              "2                 2374.335938                  709.585938   \n",
              "3                 2368.539062                  715.617188   \n",
              "4                    0.000000                    0.000000   \n",
              "5                    0.000000                    0.000000   \n",
              "6                    0.000000                    0.000000   \n",
              "7                 2346.226562                  756.429688   \n",
              "8                    0.000000                    0.000000   \n",
              "9                 2346.917969                  754.628906   \n",
              "\n",
              "   closest_person_right_knee_x  closest_person_right_knee_y  \\\n",
              "0                     0.000000                     0.000000   \n",
              "1                     0.000000                     0.000000   \n",
              "2                  2381.250000                   701.750000   \n",
              "3                  2286.554688                   749.570312   \n",
              "4                     0.000000                     0.000000   \n",
              "5                     0.000000                     0.000000   \n",
              "6                     0.000000                     0.000000   \n",
              "7                  2287.867188                   761.351562   \n",
              "8                     0.000000                     0.000000   \n",
              "9                  2363.980469                   753.207031   \n",
              "\n",
              "   closest_person_left_ankle_x  closest_person_left_ankle_y  \\\n",
              "0                     0.000000                     0.000000   \n",
              "1                     0.000000                     0.000000   \n",
              "2                  2289.523438                   748.304688   \n",
              "3                  2294.835938                   730.523438   \n",
              "4                     0.000000                     0.000000   \n",
              "5                     0.000000                     0.000000   \n",
              "6                     0.000000                     0.000000   \n",
              "7                  2315.289062                   796.507812   \n",
              "8                     0.000000                     0.000000   \n",
              "9                  2315.636719                   795.152344   \n",
              "\n",
              "   closest_person_right_ankle_x  closest_person_right_ankle_y  \n",
              "0                      0.000000                      0.000000  \n",
              "1                      0.000000                      0.000000  \n",
              "2                   2289.523438                    748.304688  \n",
              "3                   2287.382812                    740.460938  \n",
              "4                      0.000000                      0.000000  \n",
              "5                      0.000000                      0.000000  \n",
              "6                      0.000000                      0.000000  \n",
              "7                   2315.289062                    796.507812  \n",
              "8                      0.000000                      0.000000  \n",
              "9                   2315.636719                    795.152344  \n",
              "\n",
              "[10 rows x 71 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-abb474e8-c85a-4b18-92d2-bccb2eb40a6f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>frame_name</th>\n",
              "      <th>target_person_label</th>\n",
              "      <th>target_person_nose_x</th>\n",
              "      <th>target_person_nose_y</th>\n",
              "      <th>target_person_left_eye_x</th>\n",
              "      <th>target_person_left_eye_y</th>\n",
              "      <th>target_person_right_eye_x</th>\n",
              "      <th>target_person_right_eye_y</th>\n",
              "      <th>target_person_left_ear_x</th>\n",
              "      <th>target_person_left_ear_y</th>\n",
              "      <th>...</th>\n",
              "      <th>closest_person_right_hip_x</th>\n",
              "      <th>closest_person_right_hip_y</th>\n",
              "      <th>closest_person_left_knee_x</th>\n",
              "      <th>closest_person_left_knee_y</th>\n",
              "      <th>closest_person_right_knee_x</th>\n",
              "      <th>closest_person_right_knee_y</th>\n",
              "      <th>closest_person_left_ankle_x</th>\n",
              "      <th>closest_person_left_ankle_y</th>\n",
              "      <th>closest_person_right_ankle_x</th>\n",
              "      <th>closest_person_right_ankle_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>frame_00000.jpg</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2129.703125</td>\n",
              "      <td>821.953125</td>\n",
              "      <td>2124.753906</td>\n",
              "      <td>831.144531</td>\n",
              "      <td>2127.582031</td>\n",
              "      <td>831.144531</td>\n",
              "      <td>2126.167969</td>\n",
              "      <td>831.144531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>frame_00001.jpg</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2214.054688</td>\n",
              "      <td>838.570312</td>\n",
              "      <td>2125.054688</td>\n",
              "      <td>838.570312</td>\n",
              "      <td>2126.445312</td>\n",
              "      <td>838.570312</td>\n",
              "      <td>2125.054688</td>\n",
              "      <td>838.570312</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>frame_00002.jpg</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2140.757812</td>\n",
              "      <td>833.632812</td>\n",
              "      <td>2089.226562</td>\n",
              "      <td>833.632812</td>\n",
              "      <td>2116.507812</td>\n",
              "      <td>833.632812</td>\n",
              "      <td>2139.242188</td>\n",
              "      <td>833.632812</td>\n",
              "      <td>...</td>\n",
              "      <td>2380.789062</td>\n",
              "      <td>705.898438</td>\n",
              "      <td>2374.335938</td>\n",
              "      <td>709.585938</td>\n",
              "      <td>2381.250000</td>\n",
              "      <td>701.750000</td>\n",
              "      <td>2289.523438</td>\n",
              "      <td>748.304688</td>\n",
              "      <td>2289.523438</td>\n",
              "      <td>748.304688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>frame_00003.jpg</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2213.507812</td>\n",
              "      <td>834.867188</td>\n",
              "      <td>2093.273438</td>\n",
              "      <td>834.867188</td>\n",
              "      <td>2096.242188</td>\n",
              "      <td>834.867188</td>\n",
              "      <td>2142.257812</td>\n",
              "      <td>834.867188</td>\n",
              "      <td>...</td>\n",
              "      <td>2374.335938</td>\n",
              "      <td>711.476562</td>\n",
              "      <td>2368.539062</td>\n",
              "      <td>715.617188</td>\n",
              "      <td>2286.554688</td>\n",
              "      <td>749.570312</td>\n",
              "      <td>2294.835938</td>\n",
              "      <td>730.523438</td>\n",
              "      <td>2287.382812</td>\n",
              "      <td>740.460938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>frame_00004.jpg</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2150.843750</td>\n",
              "      <td>824.593750</td>\n",
              "      <td>2123.117188</td>\n",
              "      <td>833.835938</td>\n",
              "      <td>2150.843750</td>\n",
              "      <td>824.593750</td>\n",
              "      <td>2147.289062</td>\n",
              "      <td>832.414062</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>frame_00005.jpg</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2146.253906</td>\n",
              "      <td>833.441406</td>\n",
              "      <td>2143.238281</td>\n",
              "      <td>833.441406</td>\n",
              "      <td>2148.515625</td>\n",
              "      <td>823.640625</td>\n",
              "      <td>2144.746094</td>\n",
              "      <td>831.933594</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>frame_00006.jpg</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2153.328125</td>\n",
              "      <td>821.953125</td>\n",
              "      <td>2125.753906</td>\n",
              "      <td>831.144531</td>\n",
              "      <td>2153.328125</td>\n",
              "      <td>821.953125</td>\n",
              "      <td>2149.792969</td>\n",
              "      <td>829.730469</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>frame_00007.jpg</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2151.421875</td>\n",
              "      <td>822.796875</td>\n",
              "      <td>2122.933594</td>\n",
              "      <td>832.292969</td>\n",
              "      <td>2151.421875</td>\n",
              "      <td>822.796875</td>\n",
              "      <td>2147.769531</td>\n",
              "      <td>830.832031</td>\n",
              "      <td>...</td>\n",
              "      <td>2293.492188</td>\n",
              "      <td>784.554688</td>\n",
              "      <td>2346.226562</td>\n",
              "      <td>756.429688</td>\n",
              "      <td>2287.867188</td>\n",
              "      <td>761.351562</td>\n",
              "      <td>2315.289062</td>\n",
              "      <td>796.507812</td>\n",
              "      <td>2315.289062</td>\n",
              "      <td>796.507812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>frame_00008.jpg</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2144.757812</td>\n",
              "      <td>832.632812</td>\n",
              "      <td>2093.226562</td>\n",
              "      <td>832.632812</td>\n",
              "      <td>2147.031250</td>\n",
              "      <td>822.781250</td>\n",
              "      <td>2143.242188</td>\n",
              "      <td>831.117188</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>frame_00009.jpg</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2150.781250</td>\n",
              "      <td>828.531250</td>\n",
              "      <td>2123.664062</td>\n",
              "      <td>837.570312</td>\n",
              "      <td>2126.445312</td>\n",
              "      <td>837.570312</td>\n",
              "      <td>2147.304688</td>\n",
              "      <td>837.570312</td>\n",
              "      <td>...</td>\n",
              "      <td>2363.269531</td>\n",
              "      <td>751.074219</td>\n",
              "      <td>2346.917969</td>\n",
              "      <td>754.628906</td>\n",
              "      <td>2363.980469</td>\n",
              "      <td>753.207031</td>\n",
              "      <td>2315.636719</td>\n",
              "      <td>795.152344</td>\n",
              "      <td>2315.636719</td>\n",
              "      <td>795.152344</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 71 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-abb474e8-c85a-4b18-92d2-bccb2eb40a6f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-abb474e8-c85a-4b18-92d2-bccb2eb40a6f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-abb474e8-c85a-4b18-92d2-bccb2eb40a6f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a02abf10-0256-49f4-8052-548d3f345903\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a02abf10-0256-49f4-8052-548d3f345903')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a02abf10-0256-49f4-8052-548d3f345903 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/01.폭행(assult)/12-6/results.csv')\n",
        "\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l=df['target_person_label'].unique\n",
        "\n",
        "l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "NAbKpk7XjtD0",
        "outputId": "a1d4c442-8816-4d4c-f3ed-3875d41dc753"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Series.unique of 0        neutral\n",
              "1        neutral\n",
              "2        neutral\n",
              "3        neutral\n",
              "4        neutral\n",
              "          ...   \n",
              "41475    neutral\n",
              "41476    neutral\n",
              "41477    neutral\n",
              "41478    neutral\n",
              "41479    neutral\n",
              "Name: target_person_label, Length: 41480, dtype: object>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.series.Series.unique</b><br/>def unique() -&gt; ArrayLike</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pandas/core/series.py</a>Return unique values of Series object.\n",
              "\n",
              "Uniques are returned in order of appearance. Hash table-based unique,\n",
              "therefore does NOT sort.\n",
              "\n",
              "Returns\n",
              "-------\n",
              "ndarray or ExtensionArray\n",
              "    The unique values returned as a NumPy array. See Notes.\n",
              "\n",
              "See Also\n",
              "--------\n",
              "Series.drop_duplicates : Return Series with duplicate values removed.\n",
              "unique : Top-level unique method for any 1-d array-like object.\n",
              "Index.unique : Return Index with unique values from an Index object.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Returns the unique values as a NumPy array. In case of an\n",
              "extension-array backed Series, a new\n",
              ":class:`~api.extensions.ExtensionArray` of that type with just\n",
              "the unique values is returned. This includes\n",
              "\n",
              "    * Categorical\n",
              "    * Period\n",
              "    * Datetime with Timezone\n",
              "    * Datetime without Timezone\n",
              "    * Timedelta\n",
              "    * Interval\n",
              "    * Sparse\n",
              "    * IntegerNA\n",
              "\n",
              "See Examples section.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; pd.Series([2, 1, 3, 3], name=&#x27;A&#x27;).unique()\n",
              "array([2, 1, 3])\n",
              "\n",
              "&gt;&gt;&gt; pd.Series([pd.Timestamp(&#x27;2016-01-01&#x27;) for _ in range(3)]).unique()\n",
              "&lt;DatetimeArray&gt;\n",
              "[&#x27;2016-01-01 00:00:00&#x27;]\n",
              "Length: 1, dtype: datetime64[ns]\n",
              "\n",
              "&gt;&gt;&gt; pd.Series([pd.Timestamp(&#x27;2016-01-01&#x27;, tz=&#x27;US/Eastern&#x27;)\n",
              "...            for _ in range(3)]).unique()\n",
              "&lt;DatetimeArray&gt;\n",
              "[&#x27;2016-01-01 00:00:00-05:00&#x27;]\n",
              "Length: 1, dtype: datetime64[ns, US/Eastern]\n",
              "\n",
              "An Categorical will return categories in the order of\n",
              "appearance and with the same dtype.\n",
              "\n",
              "&gt;&gt;&gt; pd.Series(pd.Categorical(list(&#x27;baabc&#x27;))).unique()\n",
              "[&#x27;b&#x27;, &#x27;a&#x27;, &#x27;c&#x27;]\n",
              "Categories (3, object): [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;]\n",
              "&gt;&gt;&gt; pd.Series(pd.Categorical(list(&#x27;baabc&#x27;), categories=list(&#x27;abc&#x27;),\n",
              "...                          ordered=True)).unique()\n",
              "[&#x27;b&#x27;, &#x27;a&#x27;, &#x27;c&#x27;]\n",
              "Categories (3, object): [&#x27;a&#x27; &lt; &#x27;b&#x27; &lt; &#x27;c&#x27;]</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 2228);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LWxNJGQz33ud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51532564-a4e6-40bc-9cc4-e919057607be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 5개 행의 길이: 0    71\n",
            "1    71\n",
            "2    71\n",
            "3    71\n",
            "4    71\n",
            "dtype: int64\n",
            "최소 길이: 71\n",
            "최대 길이: 71\n",
            "평균 길이: 71.0\n",
            "전체 데이터 길이: 41480\n"
          ]
        }
      ],
      "source": [
        "row_lengths = df.apply(len, axis=1)\n",
        "\n",
        "# 첫 5개의 행의 길이 출력\n",
        "print(\"첫 5개 행의 길이:\", row_lengths.head())\n",
        "\n",
        "# 전체 데이터에 대한 통계(최소, 최대, 평균 길이) 확인\n",
        "print(\"최소 길이:\", row_lengths.min())\n",
        "print(\"최대 길이:\", row_lengths.max())\n",
        "print(\"평균 길이:\", row_lengths.mean())\n",
        "\n",
        "# 전체 데이터 길이 확인\n",
        "print(\"전체 데이터 길이:\", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIeX7mxq66f_"
      },
      "outputs": [],
      "source": [
        "df2 = pd.read_csv('/content/drive/MyDrive/논문주제/Final_project/NonFight_pose2_keypoint.csv')\n",
        "\n",
        "df2.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNtnurXG8cqE"
      },
      "outputs": [],
      "source": [
        "df2.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW74vseG7Iu6"
      },
      "outputs": [],
      "source": [
        "row_lengths2 = df2.apply(len, axis=1)\n",
        "\n",
        "# 첫 5개의 행의 길이 출력\n",
        "print(\"첫 5개 행의 길이:\", row_lengths2.head())\n",
        "\n",
        "# 전체 데이터에 대한 통계(최소, 최대, 평균 길이) 확인\n",
        "print(\"최소 길이:\", row_lengths2.min())\n",
        "print(\"최대 길이:\", row_lengths2.max())\n",
        "print(\"평균 길이:\", row_lengths2.mean())\n",
        "\n",
        "# 전체 데이터 길이 확인\n",
        "print(\"전체 데이터 길이:\", len(df2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rej-fxSC7QlX"
      },
      "outputs": [],
      "source": [
        "df2['closest_person_label'] = df2['closest_person_label'].apply(lambda x: 'not' + x if isinstance(x, str) else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnga3jT68DEK"
      },
      "outputs": [],
      "source": [
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0baEdoaH8k0T"
      },
      "outputs": [],
      "source": [
        "df2.to_csv('/content/drive/MyDrive/논문주제/Final_project/NonFight_pose2_keypoint.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2iMARrZNi0Kn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45076c91-890a-4aa4-cf82-536187739b02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(41480, 71)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 각 파일 경로를 리스트로 저장\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/01.폭행(assult)/12-6/results.csv',\n",
        "]\n",
        "\n",
        "# 빈 데이터프레임 생성\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# 각 파일을 읽고 데이터프레임에 추가\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "\n",
        "# 결과 출력\n",
        "print(combined_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QP0YNUNjmyUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba9763b5-c5aa-4cfb-c284-6039160a22bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        frame_name target_person_label  target_person_left_shoulder_x  \\\n",
            "0  frame_00000.jpg             neutral                    2127.582031   \n",
            "1  frame_00001.jpg             neutral                    2126.445312   \n",
            "2  frame_00002.jpg             neutral                    2043.000000   \n",
            "3  frame_00003.jpg             neutral                    2119.992188   \n",
            "4  frame_00004.jpg             neutral                    2125.960938   \n",
            "\n",
            "   target_person_left_shoulder_y  target_person_right_shoulder_x  \\\n",
            "0                     831.144531                     2126.167969   \n",
            "1                     838.570312                     2084.726562   \n",
            "2                     832.875000                     2090.742188   \n",
            "3                     834.867188                     2094.757812   \n",
            "4                     833.835938                     2083.304688   \n",
            "\n",
            "   target_person_right_shoulder_y  target_person_left_elbow_x  \\\n",
            "0                      829.730469                 2127.582031   \n",
            "1                      890.023438                 2126.445312   \n",
            "2                      832.117188                 2092.257812   \n",
            "3                      833.382812                 2096.242188   \n",
            "4                      840.945312                 2125.960938   \n",
            "\n",
            "   target_person_left_elbow_y  target_person_right_elbow_x  \\\n",
            "0                  831.144531                  2127.582031   \n",
            "1                  838.570312                  2126.445312   \n",
            "2                  833.632812                  2140.757812   \n",
            "3                  834.867188                  2214.250000   \n",
            "4                  833.835938                  2148.710938   \n",
            "\n",
            "   target_person_right_elbow_y  ...  closest_person_right_hip_x  \\\n",
            "0                   831.144531  ...                    0.000000   \n",
            "1                   838.570312  ...                    0.000000   \n",
            "2                   833.632812  ...                 2380.789062   \n",
            "3                   825.218750  ...                 2374.335938   \n",
            "4                   833.835938  ...                    0.000000   \n",
            "\n",
            "   closest_person_right_hip_y  closest_person_left_knee_x  \\\n",
            "0                    0.000000                    0.000000   \n",
            "1                    0.000000                    0.000000   \n",
            "2                  705.898438                 2374.335938   \n",
            "3                  711.476562                 2368.539062   \n",
            "4                    0.000000                    0.000000   \n",
            "\n",
            "   closest_person_left_knee_y  closest_person_right_knee_x  \\\n",
            "0                    0.000000                     0.000000   \n",
            "1                    0.000000                     0.000000   \n",
            "2                  709.585938                  2381.250000   \n",
            "3                  715.617188                  2286.554688   \n",
            "4                    0.000000                     0.000000   \n",
            "\n",
            "   closest_person_right_knee_y  closest_person_left_ankle_x  \\\n",
            "0                     0.000000                     0.000000   \n",
            "1                     0.000000                     0.000000   \n",
            "2                   701.750000                  2289.523438   \n",
            "3                   749.570312                  2294.835938   \n",
            "4                     0.000000                     0.000000   \n",
            "\n",
            "   closest_person_left_ankle_y  closest_person_right_ankle_x  \\\n",
            "0                     0.000000                      0.000000   \n",
            "1                     0.000000                      0.000000   \n",
            "2                   748.304688                   2289.523438   \n",
            "3                   730.523438                   2287.382812   \n",
            "4                     0.000000                      0.000000   \n",
            "\n",
            "   closest_person_right_ankle_y  \n",
            "0                      0.000000  \n",
            "1                      0.000000  \n",
            "2                    748.304688  \n",
            "3                    740.460938  \n",
            "4                      0.000000  \n",
            "\n",
            "[5 rows x 51 columns]\n"
          ]
        }
      ],
      "source": [
        "# 얼굴 관련 키포인트 제거\n",
        "columns_to_remove = [\n",
        "    'target_person_nose_x', 'target_person_nose_y', 'target_person_left_eye_x', 'target_person_left_eye_y',\n",
        "    'target_person_right_eye_x', 'target_person_right_eye_y', 'target_person_left_ear_x', 'target_person_left_ear_y',\n",
        "    'target_person_right_ear_x', 'target_person_right_ear_y',\n",
        "    'closest_person_nose_x', 'closest_person_nose_y', 'closest_person_left_eye_x', 'closest_person_left_eye_y',\n",
        "    'closest_person_right_eye_x', 'closest_person_right_eye_y', 'closest_person_left_ear_x', 'closest_person_left_ear_y',\n",
        "    'closest_person_right_ear_x', 'closest_person_right_ear_y'\n",
        "]\n",
        "\n",
        "\n",
        "# 데이터프레임에서 해당 컬럼 제거\n",
        "combined_df = combined_df.drop(columns=columns_to_remove)\n",
        "\n",
        "# 결과 확인\n",
        "print(combined_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "l4llypaim2v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea978fb-adc9-48a3-966e-35332a5ee84a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X의 shape: (40980, 20, 48)\n",
            "y의 shape: (40980, 2)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 얼굴 관련 키포인트 제거 후 남은 keypoints 리스트 정의\n",
        "keypoints = [\n",
        "    'target_person_left_shoulder_x', 'target_person_left_shoulder_y',\n",
        "    'target_person_right_shoulder_x', 'target_person_right_shoulder_y',\n",
        "    'target_person_left_elbow_x', 'target_person_left_elbow_y',\n",
        "    'target_person_right_elbow_x', 'target_person_right_elbow_y',\n",
        "    'target_person_left_wrist_x', 'target_person_left_wrist_y',\n",
        "    'target_person_right_wrist_x', 'target_person_right_wrist_y',\n",
        "    'target_person_left_hip_x', 'target_person_left_hip_y',\n",
        "    'target_person_right_hip_x', 'target_person_right_hip_y',\n",
        "    'target_person_left_knee_x', 'target_person_left_knee_y',\n",
        "    'target_person_right_knee_x', 'target_person_right_knee_y',\n",
        "    'target_person_left_ankle_x', 'target_person_left_ankle_y',\n",
        "    'target_person_right_ankle_x', 'target_person_right_ankle_y',\n",
        "    'closest_person_left_shoulder_x', 'closest_person_left_shoulder_y',\n",
        "    'closest_person_right_shoulder_x', 'closest_person_right_shoulder_y',\n",
        "    'closest_person_left_elbow_x', 'closest_person_left_elbow_y',\n",
        "    'closest_person_right_elbow_x', 'closest_person_right_elbow_y',\n",
        "    'closest_person_left_wrist_x', 'closest_person_left_wrist_y',\n",
        "    'closest_person_right_wrist_x', 'closest_person_right_wrist_y',\n",
        "    'closest_person_left_hip_x', 'closest_person_left_hip_y',\n",
        "    'closest_person_right_hip_x', 'closest_person_right_hip_y',\n",
        "    'closest_person_left_knee_x', 'closest_person_left_knee_y',\n",
        "    'closest_person_right_knee_x', 'closest_person_right_knee_y',\n",
        "    'closest_person_left_ankle_x', 'closest_person_left_ankle_y',\n",
        "    'closest_person_right_ankle_x', 'closest_person_right_ankle_y'\n",
        "]\n",
        "\n",
        "# 시퀀스 길이 설정\n",
        "n_timesteps = 20\n",
        "\n",
        "# 레이블을 원-핫 인코딩하는 함수\n",
        "def one_hot_encode(labels):\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded = np.array([label_map[label] for label in labels])\n",
        "    return np.eye(len(unique_labels))[encoded]\n",
        "\n",
        "# 시퀀스 데이터를 생성하는 함수\n",
        "def create_sequences(data, seq_length):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data) - seq_length):\n",
        "        seq = data.iloc[i:i + seq_length]\n",
        "        label = data.iloc[i + seq_length - 1]['target_person_label']\n",
        "\n",
        "        # 필요 없는 컬럼을 제외하고 시퀀스 저장\n",
        "        X.append(seq[keypoints].values)\n",
        "        y.append(label)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 30,000개의 로우만 사용하도록 데이터프레임을 슬라이스\n",
        "subset_df = combined_df.iloc[:41000]\n",
        "\n",
        "# 시퀀스 생성\n",
        "X, y = create_sequences(subset_df, n_timesteps)\n",
        "\n",
        "# 레이블 원-핫 인코딩\n",
        "y = one_hot_encode(y)\n",
        "\n",
        "# 입력 데이터와 출력 데이터의 shape 확인\n",
        "print(\"X의 shape:\", X.shape)\n",
        "print(\"y의 shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6ndyhGnRafiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ecc4d6-8d37-4470-fe7a-0920369cf520"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32784, 20, 48)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 학습 데이터와 테스트 데이터 분리\n",
        "# train test split\n",
        "# 학습 데이터와 테스트 데이터 분리 (stratify 옵션 제거)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, shuffle=True, random_state=7461\n",
        ")\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4xweJLAHh9XR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_dataset(X, y, batch_size=32, shuffle=True):\n",
        "    # 데이터셋 객체 생성\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "\n",
        "    # 데이터셋 섞기\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(X))\n",
        "\n",
        "    # 배치 크기로 분할\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# 배치 사이즈 설정\n",
        "batch_size = 64\n",
        "\n",
        "# 데이터 세트 생성\n",
        "dataset_train = create_dataset(X_train, y_train, batch_size)\n",
        "dataset_test = create_dataset(X_test, y_test, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Uw5497dFhboA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcdfacc6-c8fb-4614-93cb-f0edd569f85d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "513"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len(dataset_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fgymogdFhsDK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3dcdcb68-6864-4173-f711-57e90095fcfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with 1 LSTM layer(s)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m90,624\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ layer_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m130\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">90,624</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ layer_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m99,266\u001b[0m (387.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">99,266</span> (387.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m99,266\u001b[0m (387.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">99,266</span> (387.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Model with 1 LSTM layer(s):\n",
            "Accuracy: 0.6789897680282593\n",
            "Precision: 0.5473669271087025\n",
            "Recall: 0.6290873596876525\n",
            "F1 Score: 0.5626479448669894\n",
            "\n",
            "Training model with 2 LSTM layer(s)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m90,624\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ layer_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m49,408\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ layer_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m130\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">90,624</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ layer_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ layer_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m144,706\u001b[0m (565.26 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">144,706</span> (565.26 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m144,706\u001b[0m (565.26 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">144,706</span> (565.26 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Model with 2 LSTM layer(s):\n",
            "Accuracy: 0.6991215348243713\n",
            "Precision: 0.5660747718898679\n",
            "Recall: 0.6293313811615422\n",
            "F1 Score: 0.5774933662479114\n",
            "\n",
            "Training model with 3 LSTM layer(s)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m90,624\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ layer_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │          \u001b[38;5;34m49,408\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ layer_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │          \u001b[38;5;34m12,416\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ layer_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │              \u001b[38;5;34m64\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m2,112\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m130\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">90,624</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ layer_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ layer_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ layer_normalization_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m155,138\u001b[0m (606.01 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">155,138</span> (606.01 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m155,138\u001b[0m (606.01 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">155,138</span> (606.01 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: early stopping\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Model with 3 LSTM layer(s):\n",
            "Accuracy: 0.7532942891120911\n",
            "Precision: 0.5723964909497687\n",
            "Recall: 0.6246949731576379\n",
            "F1 Score: 0.5839288454779613\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, LayerNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# 모델 파라미터 설정\n",
        "input_shape = (X.shape[1], X.shape[2])  # 입력 시퀀스의 형태 (시퀀스 길이, 특성 수)\n",
        "num_classes = y.shape[1]  # 출력 클래스 수\n",
        "\n",
        "# Early stopping 설정\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0.001,\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# 모델 생성 함수 정의\n",
        "def build_model(lstm_layers, input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM layers\n",
        "    units = [128, 64, 32]  # 각 LSTM 레이어의 유닛 수\n",
        "    for i in range(lstm_layers):\n",
        "        return_sequences = i < lstm_layers - 1  # 마지막 LSTM layer는 return_sequences=False\n",
        "        model.add(LSTM(units[i], return_sequences=return_sequences, activation='tanh', input_shape=input_shape if i == 0 else None))\n",
        "        model.add(LayerNormalization())\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "    # Dense layers\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# LSTM layer를 1, 2, 3개 사용하는 모델을 각각 생성 및 훈련\n",
        "models = {}\n",
        "for lstm_layers in [1, 2, 3]:\n",
        "    print(f\"Training model with {lstm_layers} LSTM layer(s)...\")\n",
        "    model = build_model(lstm_layers, input_shape, num_classes)\n",
        "    model.summary()\n",
        "    history = model.fit(dataset_train, epochs=100, callbacks=[early_stopping], validation_data=dataset_test, verbose=0)\n",
        "\n",
        "    # 검증 데이터에 대한 예측 수행\n",
        "    y_pred = model.predict(dataset_test)\n",
        "\n",
        "    # 예측된 확률을 클래스 레이블로 변환\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)  # 실제 클래스 레이블\n",
        "\n",
        "    # 정밀도, 재현율, F1 점수 계산\n",
        "    precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "\n",
        "    # 모델 결과 저장\n",
        "    models[f'{lstm_layers}_LSTM'] = {\n",
        "        'accuracy': history.history['val_accuracy'][-1],\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "    print(f\"Model with {lstm_layers} LSTM layer(s):\")\n",
        "    print(f\"Accuracy: {history.history['val_accuracy'][-1]}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DkO2HmAlZ9C8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c49bd0d2-f36b-47f0-ba8d-a92e3aa7f18c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 47784 (\\N{HANGUL SYLLABLE MO}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 45944 (\\N{HANGUL SYLLABLE DEL}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49457 (\\N{HANGUL SYLLABLE SEONG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 45733 (\\N{HANGUL SYLLABLE NEUNG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 47112 (\\N{HANGUL SYLLABLE RE}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50612 (\\N{HANGUL SYLLABLE EO}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 모델의 성능 지표를 시각화하기 위한 함수 정의\n",
        "def plot_model_performance(models):\n",
        "    lstm_layers = ['1_LSTM', '2_LSTM', '3_LSTM']  # LSTM 레이어 수에 따른 키 설정\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']  # 시각화할 성능 지표\n",
        "\n",
        "    for metric in metrics:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        values = [models[layer][metric] for layer in lstm_layers]\n",
        "        plt.plot(lstm_layers, values, marker='o', linestyle='-', label=metric)\n",
        "        plt.title(f'모델 성능 - {metric.capitalize()}')\n",
        "        plt.xlabel('LSTM 레이어 수')\n",
        "        plt.ylabel(metric.capitalize())\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "# 모델의 성능 지표를 시각화\n",
        "plot_model_performance(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hSPvJKugaM9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9ce80c-5806-492f-c0c1-e0bfa382668b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# 모델을 HDF5 파일 형식으로 저장\n",
        "model.save('/content/drive/MyDrive/논문주제/Final_project/xml_lstm.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Mhn_5IOmhv1k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "7303b81e-a01c-4bbb-a94b-3a93da774bdf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 훈련 과정 시각화 (선택적)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'], label='Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.plot(history.history['loss'], label='Loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.title('lstm3')\n",
        "plt.ylabel('Value')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP9o6dy9dvdb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBPjOByodxIf"
      },
      "source": [
        "영상테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jbMJ1mpiHA6"
      },
      "outputs": [],
      "source": [
        "!wget -q -O /content/DejaVuSans-Bold.ttf https://github.com/dejavu-fonts/dejavu-fonts/raw/master/ttf/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CUKZZzLeBLM"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab.patches import cv2_imshow  # 코랩 환경에서 이미지 표시를 위한 모듈\n",
        "import os\n",
        "\n",
        "# 영상 파일 위치 지정\n",
        "video_file = '/content/drive/MyDrive/논문주제/Final_project/Dataset/학교폭력 영상/F_176_1_0_0_0.mp4'\n",
        "\n",
        "# YOLO 모델 정의 (학습 당시에는 v8x로 진행)\n",
        "yolo_model = YOLO('yolov8n.pt')\n",
        "\n",
        "# LSTM 모델 정의\n",
        "lstm_model = load_model('/content/drive/MyDrive/논문주제/Final_project/my_model_lstm2.h5')\n",
        "\n",
        "# 시퀀스 정의\n",
        "sequences = []\n",
        "\n",
        "# 시퀀스 길이\n",
        "sequence_length = 8\n",
        "\n",
        "# 동작 라벨 정의\n",
        "actions = [\n",
        "    \"falling\", \"attack\", \"defense\", \"aggressive\", \"punch\", \"push\",\n",
        "    \"grab_collar\", \"kicking\", \"elbow_strike\", \"shoving\",\n",
        "    \"choking\", \"stomping\", \"crouching\", \"neutral\"\n",
        "]\n",
        "\n",
        "# 위험한 동작 라벨 정의\n",
        "dangerous_actions = [\n",
        "    \"falling\", \"attack\", \"aggressive\", \"punch\", \"grab_collar\",\n",
        "    \"kicking\", \"elbow_strike\", \"shoving\", \"choking\", \"stomping\"\n",
        "]\n",
        "\n",
        "# 이전에 그려진 박스의 좌표를 저장할 변수\n",
        "prev_box = None\n",
        "\n",
        "# 동영상 파일로부터 비디오 캡처 객체 생성\n",
        "cap = cv2.VideoCapture(video_file)\n",
        "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
        "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
        "\n",
        "\n",
        "def extract_keypoint(keypoints):\n",
        "    # keypoints: [x, y, confidence, ...]\n",
        "    return keypoints[:, :2].flatten().tolist()  # x, y 좌표만 사용\n",
        "\n",
        "def find_closest_pairs(boxes, labels, keypoints):\n",
        "    \"\"\"\n",
        "    주어진 조건에 따라 두 사람 간의 가장 적합한 바운딩 박스를 반환합니다.\n",
        "    \"\"\"\n",
        "    min_distance = float('inf')\n",
        "    closest_pair = None\n",
        "\n",
        "    # 쓰러진 사람이 있는지 확인\n",
        "    falling_indices = [i for i, label in enumerate(labels) if label == 'falling']\n",
        "    if falling_indices:\n",
        "        fallen_index = falling_indices[0]\n",
        "        fallen_keypoints = keypoints[fallen_index]\n",
        "\n",
        "        for i, kp in enumerate(keypoints):\n",
        "            if i != fallen_index:\n",
        "                distance = np.linalg.norm(np.array(fallen_keypoints[GetKeypoint.LEFT_HIP*2:GetKeypoint.LEFT_HIP*2+2]) -\n",
        "                                          np.array(kp[GetKeypoint.LEFT_HIP*2:GetKeypoint.LEFT_HIP*2+2]))\n",
        "                if distance < min_distance:\n",
        "                    min_distance = distance\n",
        "                    closest_pair = (fallen_index, i)\n",
        "\n",
        "    # 웅크리고 있는 사람이 있는지 확인\n",
        "    elif not closest_pair:\n",
        "        crouching_indices = [i for i, label in enumerate(labels) if label == 'crouching']\n",
        "        if crouching_indices:\n",
        "            crouching_index = crouching_indices[0]\n",
        "            crouching_keypoints = keypoints[crouching_index]\n",
        "\n",
        "            for i, kp in enumerate(keypoints):\n",
        "                if i != crouching_index:\n",
        "                    distance = np.linalg.norm(np.array(crouching_keypoints[GetKeypoint.LEFT_HIP*2:GetKeypoint.LEFT_HIP*2+2]) -\n",
        "                                              np.array(kp[GetKeypoint.LEFT_HIP*2:GetKeypoint.LEFT_HIP*2+2]))\n",
        "                    if distance < min_distance:\n",
        "                        min_distance = distance\n",
        "                        closest_pair = (crouching_index, i)\n",
        "\n",
        "    # 두 사람이 있는 경우, 가장 가까운 두 사람을 선택\n",
        "    if not closest_pair:\n",
        "        for i in range(len(boxes)):\n",
        "            for j in range(i + 1, len(boxes)):\n",
        "                center_i = [(boxes[i][0] + boxes[i][2]) / 2, (boxes[i][1] + boxes[i][3]) / 2]\n",
        "                center_j = [(boxes[j][0] + boxes[j][2]) / 2, (boxes[j][1] + boxes[j][3]) / 2]\n",
        "                distance = np.linalg.norm(np.array(center_i) - np.array(center_j))\n",
        "\n",
        "                if distance < min_distance:\n",
        "                    min_distance = distance\n",
        "                    closest_pair = (i, j)\n",
        "\n",
        "    return [boxes[closest_pair[0]], boxes[closest_pair[1]]]\n",
        "\n",
        "def create_largest_box(box1, box2):\n",
        "    \"\"\"\n",
        "    두 사람의 바운딩 박스를 감싸는 가장 큰 바운딩 박스를 생성합니다.\n",
        "    \"\"\"\n",
        "    x1 = min(box1[0], box2[0])\n",
        "    y1 = min(box1[1], box2[1])\n",
        "    x2 = max(box1[2], box2[2])\n",
        "    y2 = max(box1[3], box2[3])\n",
        "    return [x1, y1, x2, y2]\n",
        "\n",
        "def calculate_font_center(box):\n",
        "    \"\"\"\n",
        "    바운딩 박스의 중앙 위치를 계산하여 텍스트를 표시할 위치를 반환합니다.\n",
        "    \"\"\"\n",
        "    center_x = (box[0] + box[2]) / 2\n",
        "    center_y = (box[1] + box[3]) / 2\n",
        "    return center_x, center_y\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if not success:\n",
        "        break\n",
        "\n",
        "    results = yolo_model(frame, save=False, classes=[0])\n",
        "\n",
        "    if not results:\n",
        "        continue\n",
        "\n",
        "    # 키포인트가 있는지 확인하고 처리\n",
        "    if results[0].keypoints is not None:\n",
        "        results_keypoint = results[0].keypoints.xyn.cpu().numpy()\n",
        "        labels = [classify_pose(extract_keypoint(kp)) for kp in results_keypoint]\n",
        "\n",
        "        if len(results_keypoint) == 1:\n",
        "            keypoint_list = extract_keypoint(results_keypoint[0])\n",
        "            keypoint_list.extend([0] * 24)\n",
        "        elif len(results_keypoint) >= 2:\n",
        "            xy_list = results[0].boxes.xyxy.cpu().numpy().tolist()\n",
        "            closest_boxes = find_closest_pairs(xy_list, labels, results_keypoint)\n",
        "            keypoint_list1 = extract_keypoint(results_keypoint[0])\n",
        "            keypoint_list2 = extract_keypoint(results_keypoint[1])\n",
        "            keypoint_list = keypoint_list1 + keypoint_list2\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        sequences.append(keypoint_list)\n",
        "\n",
        "    if len(sequences) == sequence_length:\n",
        "        sequences_array = np.array(sequences).reshape(1, sequence_length, -1)\n",
        "        predictions = lstm_model.predict(sequences_array)\n",
        "\n",
        "        action_index = np.argmax(predictions)\n",
        "        action_name = actions[action_index]\n",
        "        max_value = np.max(predictions)\n",
        "\n",
        "        if action_name in dangerous_actions and max_value > 0.9:\n",
        "            box_color = (0, 0, 255)\n",
        "            last_dangerous_action_detected = action_name\n",
        "        elif max_value > 0.5:\n",
        "            box_color = (0, 128, 255)\n",
        "            last_dangerous_action_detected = None\n",
        "        else:\n",
        "            box_color = (0, 255, 0)\n",
        "            last_dangerous_action_detected = None\n",
        "\n",
        "        largest_box = create_largest_box(*closest_boxes)\n",
        "\n",
        "        prev_box = largest_box\n",
        "        sequences.clear()\n",
        "\n",
        "    if prev_box is not None:\n",
        "        pil_im = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        draw = ImageDraw.Draw(pil_im)\n",
        "\n",
        "        draw.rectangle(((prev_box[0], prev_box[1]), (prev_box[2], prev_box[3])), outline=box_color, width=5)\n",
        "        draw.text((prev_box[0], prev_box[1] - 40), action_name, font=font, fill=box_color)\n",
        "\n",
        "        if last_dangerous_action_detected:\n",
        "            danger_text = f\"Danger! {last_dangerous_action_detected} detected!\"\n",
        "            draw.text((50, 100), danger_text, font=font, fill=(255, 0, 0))\n",
        "\n",
        "        frame = cv2.cvtColor(np.array(pil_im), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    cv2_imshow(frame)  # 코랩 환경에서는 cv2_imshow를 사용\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YupojcfklYn1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}