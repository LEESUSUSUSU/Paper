{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/somoon0422/Paper/blob/main/alphapose_lstm_re0911.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITXFPsQFu_8P"
      },
      "source": [
        "**I modified the Pillow installation part of the AlphaPose Colab example program.**\n",
        "\n",
        "**This is not thoth000's original program.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cxMU0dmlnCT",
        "outputId": "79ecfd97-9c29-4f8f-b3c3-d8e88980a048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml==5.2\n",
            "  Using cached PyYAML-5.2.tar.gz (265 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "2.4.0+cu121\n",
            "6.0.2\n",
            "1.13.1\n"
          ]
        }
      ],
      "source": [
        "! pip install pyyaml==5.2\n",
        "! pip install scipy\n",
        "! pip install numpy\n",
        "! pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import yaml, scipy, os\n",
        "print(yaml.__version__)\n",
        "print(scipy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VBhQTOSoWab",
        "outputId": "b90ae4e2-4879-4f27-f6bc-c403266fa0f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AlphaPose'...\n",
            "remote: Enumerating objects: 2749, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 2749 (delta 4), reused 1 (delta 0), pack-reused 2739 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2749/2749), 118.82 MiB | 29.40 MiB/s, done.\n",
            "Resolving deltas: 100% (1379/1379), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/AlphaPose\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/MVIG-SJTU/AlphaPose.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkkNtO8qolbz",
        "outputId": "1f87beb0-f14d-463b-b6be-a36be75dceb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (3.0.11)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libyaml-doc\n",
            "The following NEW packages will be installed:\n",
            "  libyaml-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 62.8 kB of archives.\n",
            "After this operation, 257 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libyaml-dev amd64 0.2.2-1build2 [62.8 kB]\n",
            "Fetched 62.8 kB in 0s (173 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libyaml-dev:amd64.\n",
            "(Reading database ... 123597 files and directories currently installed.)\n",
            "Preparing to unpack .../libyaml-dev_0.2.2-1build2_amd64.deb ...\n",
            "Unpacking libyaml-dev:amd64 (0.2.2-1build2) ...\n",
            "Setting up libyaml-dev:amd64 (0.2.2-1build2) ...\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install cython\n",
        "!sudo apt-get install libyaml-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-Gw3k4coyFD",
        "outputId": "355b06d5-4416-4d41-e653-8227d3891c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AlphaPose\n",
            "Compiling detector/nms/src/soft_nms_cpu.pyx because it changed.\n",
            "[1/1] Cythonizing detector/nms/src/soft_nms_cpu.pyx\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/__init__.py:85: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Requirements should be satisfied by a PEP 517 installer.\n",
            "        If you are using pip, you can try `pip install --use-pep517`.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  dist.fetch_build_eggs(dist.setup_requires)\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-cpython-310\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers\n",
            "copying trackers/tracker_cfg.py -> build/lib.linux-x86_64-cpython-310/trackers\n",
            "copying trackers/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers\n",
            "copying trackers/tracker_api.py -> build/lib.linux-x86_64-cpython-310/trackers\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose\n",
            "copying alphapose/opt.py -> build/lib.linux-x86_64-cpython-310/alphapose\n",
            "copying alphapose/version.py -> build/lib.linux-x86_64-cpython-310/alphapose\n",
            "copying alphapose/__init__.py -> build/lib.linux-x86_64-cpython-310/alphapose\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/tracking\n",
            "copying trackers/tracking/basetrack.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking\n",
            "copying trackers/tracking/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking\n",
            "copying trackers/tracking/matching.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/osnet.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/ResNet.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/resnet_fc.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/ResBnLin.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/bn_linear.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/osnet_ain.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/net_utils.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "copying trackers/tracking/utils/io.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "copying trackers/tracking/utils/parse_config.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "copying trackers/tracking/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "copying trackers/tracking/utils/utils.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "copying trackers/tracking/utils/nms.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "copying trackers/tracking/utils/timer.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "copying trackers/tracking/utils/kalman_filter.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/classification\n",
            "copying trackers/ReidModels/classification/classifier.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/classification\n",
            "copying trackers/ReidModels/classification/rfcn_cls.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/classification\n",
            "copying trackers/ReidModels/classification/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/classification\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling\n",
            "copying trackers/ReidModels/psroi_pooling/build.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling\n",
            "copying trackers/ReidModels/psroi_pooling/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/reid\n",
            "copying trackers/ReidModels/reid/image_part_aligned.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/reid\n",
            "copying trackers/ReidModels/reid/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/reid\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/backbone\n",
            "copying trackers/ReidModels/backbone/googlenet.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/backbone\n",
            "copying trackers/ReidModels/backbone/sqeezenet.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/backbone\n",
            "copying trackers/ReidModels/backbone/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/backbone\n",
            "copying trackers/ReidModels/backbone/lrn.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/backbone\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/_ext\n",
            "copying trackers/ReidModels/psroi_pooling/_ext/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/_ext\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/modules\n",
            "copying trackers/ReidModels/psroi_pooling/modules/psroi_pool.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/modules\n",
            "copying trackers/ReidModels/psroi_pooling/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/functions\n",
            "copying trackers/ReidModels/psroi_pooling/functions/psroi_pooling.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/functions\n",
            "copying trackers/ReidModels/psroi_pooling/functions/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/functions\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/_ext/psroi_pooling\n",
            "copying trackers/ReidModels/psroi_pooling/_ext/psroi_pooling/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/_ext/psroi_pooling\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/fastpose_duc_dense.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/fastpose.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/hardnet.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/simple3dposeSMPLWithCam.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/hrnet.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/simplepose.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/criterion.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/fastpose_duc.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/__init__.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/builder.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_136.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_coco_wholebody_136.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/coco_wholebody.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_coco_wholebody_26_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_26_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_68_noface_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/coco_wholebody_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/concat_dataset.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_coco_wholebody_136_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_68_noface.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_coco_wholebody_26.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/mpii.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_136_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/single_hand_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/coco_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/__init__.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/custom.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/mscoco.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_26.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/single_hand.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/render_pytorch3d.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/webcam_detector.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/registry.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/env.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/metrics.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/pPose_nms.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/writer_smpl.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/vis.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/logger.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/file_detector.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/config.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/bbox.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/detector.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/writer.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/transforms.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose/utils/presets\n",
            "copying alphapose/utils/presets/simple_transform_3d_smpl.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils/presets\n",
            "copying alphapose/utils/presets/simple_transform.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils/presets\n",
            "copying alphapose/utils/presets/__init__.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils/presets\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose/utils/roi_align\n",
            "copying alphapose/utils/roi_align/roi_align.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils/roi_align\n",
            "copying alphapose/utils/roi_align/__init__.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils/roi_align\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:495: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "building 'detector.nms.soft_nms_cpu' extension\n",
            "creating build/temp.linux-x86_64-cpython-310\n",
            "creating build/temp.linux-x86_64-cpython-310/detector\n",
            "creating build/temp.linux-x86_64-cpython-310/detector/nms\n",
            "creating build/temp.linux-x86_64-cpython-310/detector/nms/src\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c detector/nms/src/soft_nms_cpu.cpp -o build/temp.linux-x86_64-cpython-310/detector/nms/src/soft_nms_cpu.o -Wno-unused-function -Wno-write-strings -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=soft_nms_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1929\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/arrayobject.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kdetector/nms/src/soft_nms_cpu.cpp:1268\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcpp\u0007-Wcpp\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-cpython-310/detector\n",
            "creating build/lib.linux-x86_64-cpython-310/detector/nms\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/detector/nms/src/soft_nms_cpu.o -L/usr/lib/x86_64-linux-gnu -o build/lib.linux-x86_64-cpython-310/detector/nms/soft_nms_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "building 'detector.nms.nms_cpu' extension\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c detector/nms/src/nms_cpu.cpp -o build/temp.linux-x86_64-cpython-310/detector/nms/src/nms_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=nms_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/detector/nms/src/nms_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/detector/nms/nms_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "building 'detector.nms.nms_cuda' extension\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c detector/nms/src/nms_cuda.cpp -o build/temp.linux-x86_64-cpython-310/detector/nms/src/nms_cuda.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=nms_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c detector/nms/src/nms_kernel.cu -o build/temp.linux-x86_64-cpython-310/detector/nms/src/nms_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=nms_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/detector/nms/src/nms_cuda.o build/temp.linux-x86_64-cpython-310/detector/nms/src/nms_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/detector/nms/nms_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "building 'alphapose.utils.roi_align.roi_align_cuda' extension\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose/utils\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose/utils/roi_align\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose/utils/roi_align/src\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c alphapose/utils/roi_align/src/roi_align_cuda.cpp -o build/temp.linux-x86_64-cpython-310/alphapose/utils/roi_align/src/roi_align_cuda.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=roi_align_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c alphapose/utils/roi_align/src/roi_align_kernel.cu -o build/temp.linux-x86_64-cpython-310/alphapose/utils/roi_align/src/roi_align_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=roi_align_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/alphapose/utils/roi_align/src/roi_align_cuda.o build/temp.linux-x86_64-cpython-310/alphapose/utils/roi_align/src/roi_align_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/alphapose/utils/roi_align/roi_align_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "building 'alphapose.models.layers.dcn.deform_conv_cuda' extension\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose/models\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose/models/layers\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c alphapose/models/layers/dcn/src/deform_conv_cuda.cpp -o build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_conv_cuda.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=deform_conv_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c alphapose/models/layers/dcn/src/deform_conv_cuda_kernel.cu -o build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_conv_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=deform_conv_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose/models/layers\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose/models/layers/dcn\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_conv_cuda.o build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_conv_cuda_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/alphapose/models/layers/dcn/deform_conv_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "building 'alphapose.models.layers.dcn.deform_pool_cuda' extension\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c alphapose/models/layers/dcn/src/deform_pool_cuda.cpp -o build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_pool_cuda.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=deform_pool_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c alphapose/models/layers/dcn/src/deform_pool_cuda_kernel.cu -o build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_pool_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=deform_pool_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_pool_cuda.o build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_pool_cuda_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/alphapose/models/layers/dcn/deform_pool_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "running develop\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:42: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running egg_info\n",
            "creating alphapose.egg-info\n",
            "writing alphapose.egg-info/PKG-INFO\n",
            "writing dependency_links to alphapose.egg-info/dependency_links.txt\n",
            "writing requirements to alphapose.egg-info/requires.txt\n",
            "writing top-level names to alphapose.egg-info/top_level.txt\n",
            "writing manifest file 'alphapose.egg-info/SOURCES.txt'\n",
            "dependency /usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
            "dependency /usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
            "dependency /usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
            "dependency /usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
            "dependency /usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
            "reading manifest file 'alphapose.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'alphapose.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "copying build/lib.linux-x86_64-cpython-310/detector/nms/soft_nms_cpu.cpython-310-x86_64-linux-gnu.so -> detector/nms\n",
            "copying build/lib.linux-x86_64-cpython-310/detector/nms/nms_cpu.cpython-310-x86_64-linux-gnu.so -> detector/nms\n",
            "copying build/lib.linux-x86_64-cpython-310/detector/nms/nms_cuda.cpython-310-x86_64-linux-gnu.so -> detector/nms\n",
            "copying build/lib.linux-x86_64-cpython-310/alphapose/utils/roi_align/roi_align_cuda.cpython-310-x86_64-linux-gnu.so -> alphapose/utils/roi_align\n",
            "copying build/lib.linux-x86_64-cpython-310/alphapose/models/layers/dcn/deform_conv_cuda.cpython-310-x86_64-linux-gnu.so -> alphapose/models/layers/dcn\n",
            "copying build/lib.linux-x86_64-cpython-310/alphapose/models/layers/dcn/deform_pool_cuda.cpython-310-x86_64-linux-gnu.so -> alphapose/models/layers/dcn\n",
            "Creating /usr/local/lib/python3.10/dist-packages/alphapose.egg-link (link to .)\n",
            "Adding alphapose 0.5.0+c60106d to easy-install.pth file\n",
            "\n",
            "Installed /content/AlphaPose\n",
            "Processing dependencies for alphapose==0.5.0+c60106d\n",
            "Searching for timm==0.1.20\n",
            "Reading https://pypi.tuna.tsinghua.edu.cn/simple/timm/\n",
            "Downloading https://pypi.tuna.tsinghua.edu.cn/packages/89/26/ba294669cc5cc4d09efd1964c8df752dc0955ac26f86bdeec582aed77d1d/timm-0.1.20-py3-none-any.whl#sha256=f63fca201f637dfdd169fb187b5c2d06b8e973d537d2517667a80e57ca1bae7a\n",
            "Best match: timm 0.1.20\n",
            "Processing timm-0.1.20-py3-none-any.whl\n",
            "Installing timm-0.1.20-py3-none-any.whl to /usr/local/lib/python3.10/dist-packages\n",
            "Adding timm 0.1.20 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/timm-0.1.20-py3.10.egg\n",
            "Searching for munkres\n",
            "Reading https://pypi.tuna.tsinghua.edu.cn/simple/munkres/\n",
            "Downloading https://pypi.tuna.tsinghua.edu.cn/packages/90/ab/0301c945a704218bc9435f0e3c88884f6b19ef234d8899fb47ce1ccfd0c9/munkres-1.1.4-py2.py3-none-any.whl#sha256=6b01867d4a8480d865aea2326e4b8f7c46431e9e55b4a2e32d989307d7bced2a\n",
            "Best match: munkres 1.1.4\n",
            "Processing munkres-1.1.4-py2.py3-none-any.whl\n",
            "Installing munkres-1.1.4-py2.py3-none-any.whl to /usr/local/lib/python3.10/dist-packages\n",
            "Adding munkres 1.1.4 to easy-install.pth file\n",
            "detected new path './timm-0.1.20-py3.10.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/munkres-1.1.4-py3.10.egg\n",
            "Searching for halpecocotools\n",
            "Reading https://pypi.tuna.tsinghua.edu.cn/simple/halpecocotools/\n",
            "Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f9/b0/f2e783a29a30cf0e66e3bbc45cdc4da6e9214bc21f4df948ee9a7d79764e/halpecocotools-0.0.0.tar.gz#sha256=8355964f7d14e69b2a555be7e116eae8979ec2a04f0eef16d140b38d04f933eb\n",
            "Best match: halpecocotools 0.0.0\n",
            "Processing halpecocotools-0.0.0.tar.gz\n",
            "Writing /tmp/easy_install-zryqv1n2/halpecocotools-0.0.0/setup.cfg\n",
            "Running halpecocotools-0.0.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-zryqv1n2/halpecocotools-0.0.0/egg-dist-tmp-ojthsqw5\n",
            "warning: no files found matching '*.pxd'\n",
            "warning: no files found matching '*.pyx'\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Main.py:381: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: /tmp/easy_install-zryqv1n2/halpecocotools-0.0.0/halpecocotools/_mask.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "Traceback (most recent call last):\n",
            "  File \"Cython/Compiler/Visitor.py\", line 182, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/ParseTreeTransforms.py\", line 3113, in visit_StatListNode\n",
            "    if not self.current_directives['remove_unreachable']:\n",
            "TypeError: 'NoneType' object is not subscriptable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 158, in save_modules\n",
            "    yield saved\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 200, in setup_context\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 261, in run_setup\n",
            "    _execfile(setup_script, ns)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 48, in _execfile\n",
            "    exec(code, globals, locals)\n",
            "  File \"/tmp/easy_install-zryqv1n2/halpecocotools-0.0.0/setup.py\", line 16, in <module>\n",
            "    \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/__init__.py\", line 108, in setup\n",
            "    return distutils.core.setup(**attrs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 184, in setup\n",
            "    return run_commands(dist)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 200, in run_commands\n",
            "    dist.run_commands()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 970, in run_commands\n",
            "    self.run_command(cmd)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/bdist_egg.py\", line 167, in run\n",
            "    cmd = self.call_command('install_lib', warn_dir=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/bdist_egg.py\", line 153, in call_command\n",
            "    self.run_command(cmdname)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "    self.distribution.run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/install_lib.py\", line 12, in run\n",
            "    self.build()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/install_lib.py\", line 110, in build\n",
            "    self.run_command('build_ext')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "    self.distribution.run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/build_ext.py\", line 93, in run\n",
            "    _build_ext.run(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\n",
            "    self.build_extensions()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/build_ext.py\", line 479, in build_extensions\n",
            "    self._build_extensions_serial()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/build_ext.py\", line 505, in _build_extensions_serial\n",
            "    self.build_extension(ext)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/build_ext.py\", line 254, in build_extension\n",
            "    _build_ext.build_extension(self, ext)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Distutils/build_ext.py\", line 130, in build_extension\n",
            "    new_ext = cythonize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Build/Dependencies.py\", line 1154, in cythonize\n",
            "    cythonize_one(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Build/Dependencies.py\", line 1300, in cythonize_one\n",
            "    result = compile_single(pyx_file, options, full_module_name=full_module_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Main.py\", line 615, in compile_single\n",
            "    return run_pipeline(source, options, full_module_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Main.py\", line 539, in run_pipeline\n",
            "    err, enddata = Pipeline.run_pipeline(pipeline, source)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Pipeline.py\", line 398, in run_pipeline\n",
            "    data = run(phase, data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Pipeline.py\", line 375, in run\n",
            "    return phase(data)\n",
            "  File \"Cython/Compiler/Visitor.py\", line 312, in Cython.Compiler.Visitor.CythonTransform.__call__\n",
            "  File \"Cython/Compiler/Visitor.py\", line 294, in Cython.Compiler.Visitor.VisitorTransform.__call__\n",
            "  File \"Cython/Compiler/Visitor.py\", line 184, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"Cython/Compiler/Visitor.py\", line 182, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"Cython/Compiler/Visitor.py\", line 322, in Cython.Compiler.Visitor.CythonTransform.visit_Node\n",
            "  File \"Cython/Compiler/Visitor.py\", line 260, in Cython.Compiler.Visitor.VisitorTransform._process_children\n",
            "  File \"Cython/Compiler/Visitor.py\", line 227, in Cython.Compiler.Visitor.TreeVisitor._visitchildren\n",
            "  File \"Cython/Compiler/Visitor.py\", line 196, in Cython.Compiler.Visitor.TreeVisitor._visitchild\n",
            "  File \"Cython/Compiler/Visitor.py\", line 190, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"Cython/Compiler/Visitor.py\", line 148, in Cython.Compiler.Visitor.TreeVisitor._raise_compiler_error\n",
            "Cython.Compiler.Errors.CompilerCrash: \n",
            "Error compiling Cython file:\n",
            "------------------------------------------------------------\n",
            "...\n",
            "# distutils: language = c\n",
            "^\n",
            "------------------------------------------------------------\n",
            "\n",
            "halpecocotools/_mask.pyx:1:0: Compiler crash in RemoveUnreachableCode\n",
            "\n",
            "ModuleNode.body = StatListNode(_mask.pyx:1:0)\n",
            "\n",
            "Compiler crash traceback from this point on:\n",
            "  File \"Cython/Compiler/Visitor.py\", line 182, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/ParseTreeTransforms.py\", line 3113, in visit_StatListNode\n",
            "    if not self.current_directives['remove_unreachable']:\n",
            "TypeError: 'NoneType' object is not subscriptable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AlphaPose/setup.py\", line 187, in <module>\n",
            "    setup(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/__init__.py\", line 108, in setup\n",
            "    return distutils.core.setup(**attrs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 184, in setup\n",
            "    return run_commands(dist)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 200, in run_commands\n",
            "    dist.run_commands()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 970, in run_commands\n",
            "    self.run_command(cmd)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py\", line 36, in run\n",
            "    self.install_for_development()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py\", line 128, in install_for_development\n",
            "    self.process_distribution(None, self.dist, not self.no_deps)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/easy_install.py\", line 785, in process_distribution\n",
            "    distros = WorkingSet([]).resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 893, in resolve\n",
            "    dist = self._resolve_dist(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 929, in _resolve_dist\n",
            "    dist = best[req.key] = env.best_match(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 1259, in best_match\n",
            "    return self.obtain(req, installer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 1295, in obtain\n",
            "    return installer(requirement) if installer else None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/easy_install.py\", line 709, in easy_install\n",
            "    return self.install_item(spec, dist.location, tmpdir, deps)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/easy_install.py\", line 734, in install_item\n",
            "    dists = self.install_eggs(spec, download, tmpdir)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/easy_install.py\", line 931, in install_eggs\n",
            "    return self.build_and_install(setup_script, setup_base)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/easy_install.py\", line 1203, in build_and_install\n",
            "    self.run_setup(setup_script, setup_base, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/easy_install.py\", line 1189, in run_setup\n",
            "    run_setup(setup_script, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 251, in run_setup\n",
            "    with setup_context(setup_dir):\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 192, in setup_context\n",
            "    with save_modules():\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 171, in save_modules\n",
            "    saved_exc.resume()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 145, in resume\n",
            "    raise exc.with_traceback(self._tb)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 158, in save_modules\n",
            "    yield saved\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 200, in setup_context\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 261, in run_setup\n",
            "    _execfile(setup_script, ns)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 48, in _execfile\n",
            "    exec(code, globals, locals)\n",
            "  File \"/tmp/easy_install-zryqv1n2/halpecocotools-0.0.0/setup.py\", line 16, in <module>\n",
            "    \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/__init__.py\", line 108, in setup\n",
            "    return distutils.core.setup(**attrs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 184, in setup\n",
            "    return run_commands(dist)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 200, in run_commands\n",
            "    dist.run_commands()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 970, in run_commands\n",
            "    self.run_command(cmd)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/bdist_egg.py\", line 167, in run\n",
            "    cmd = self.call_command('install_lib', warn_dir=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/bdist_egg.py\", line 153, in call_command\n",
            "    self.run_command(cmdname)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "    self.distribution.run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/install_lib.py\", line 12, in run\n",
            "    self.build()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/install_lib.py\", line 110, in build\n",
            "    self.run_command('build_ext')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "    self.distribution.run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/build_ext.py\", line 93, in run\n",
            "    _build_ext.run(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\n",
            "    self.build_extensions()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/build_ext.py\", line 479, in build_extensions\n",
            "    self._build_extensions_serial()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/build_ext.py\", line 505, in _build_extensions_serial\n",
            "    self.build_extension(ext)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/build_ext.py\", line 254, in build_extension\n",
            "    _build_ext.build_extension(self, ext)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Distutils/build_ext.py\", line 130, in build_extension\n",
            "    new_ext = cythonize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Build/Dependencies.py\", line 1154, in cythonize\n",
            "    cythonize_one(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Build/Dependencies.py\", line 1300, in cythonize_one\n",
            "    result = compile_single(pyx_file, options, full_module_name=full_module_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Main.py\", line 615, in compile_single\n",
            "    return run_pipeline(source, options, full_module_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Main.py\", line 539, in run_pipeline\n",
            "    err, enddata = Pipeline.run_pipeline(pipeline, source)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Pipeline.py\", line 398, in run_pipeline\n",
            "    data = run(phase, data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Pipeline.py\", line 375, in run\n",
            "    return phase(data)\n",
            "  File \"Cython/Compiler/Visitor.py\", line 312, in Cython.Compiler.Visitor.CythonTransform.__call__\n",
            "  File \"Cython/Compiler/Visitor.py\", line 294, in Cython.Compiler.Visitor.VisitorTransform.__call__\n",
            "  File \"Cython/Compiler/Visitor.py\", line 184, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"Cython/Compiler/Visitor.py\", line 182, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"Cython/Compiler/Visitor.py\", line 322, in Cython.Compiler.Visitor.CythonTransform.visit_Node\n",
            "  File \"Cython/Compiler/Visitor.py\", line 260, in Cython.Compiler.Visitor.VisitorTransform._process_children\n",
            "  File \"Cython/Compiler/Visitor.py\", line 227, in Cython.Compiler.Visitor.TreeVisitor._visitchildren\n",
            "  File \"Cython/Compiler/Visitor.py\", line 196, in Cython.Compiler.Visitor.TreeVisitor._visitchild\n",
            "  File \"Cython/Compiler/Visitor.py\", line 190, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"Cython/Compiler/Visitor.py\", line 148, in Cython.Compiler.Visitor.TreeVisitor._raise_compiler_error\n",
            "setuptools.sandbox.UnpickleableException: CompilerCrash((<FileSourceDescriptor:/tmp/easy_install-zryqv1n2/halpecocotools-0.0.0/halpecocotools/_mask.pyx>, 1, 0), 'RemoveUnreachableCode', 'Compiler crash in RemoveUnreachableCode\\n\\nModuleNode.body = StatListNode(_mask.pyx:1:0)\\n\\nCompiler crash traceback from this point on:\\n  File \"Cython/Compiler/Visitor.py\", line 182, in Cython.Compiler.Visitor.TreeVisitor._visit\\n  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/ParseTreeTransforms.py\", line 3113, in visit_StatListNode\\n    if not self.current_directives[\\'remove_unreachable\\']:\\nTypeError: \\'NoneType\\' object is not subscriptable', TypeError(\"'NoneType' object is not subscriptable\"), <traceback object at 0x7c971e266400>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "os.chdir('/content/AlphaPose')\n",
        "print(os.getcwd())\n",
        "! python setup.py build develop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9tg0Z2RznSv",
        "outputId": "527c5637-5c93-4f52-ee27-d472b73316c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "! mkdir /content/AlphaPose/detector/yolo/data\n",
        "file_id = '1D47msNOOiJKvPOXlnpyzdKA3k6E97NTC'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('/content/AlphaPose/detector/yolo/data/yolov3-spp.weights')\n",
        "\n",
        "! mkdir /content/AlphaPose/detector/tracker/data\n",
        "file_id = '1nlnuYfGNuHWZztQHXwVZSL_FvfE551pA'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('/content/AlphaPose/detector/tracker/data/JDE-1088x608-uncertainty')\n",
        "\n",
        "file_id = '1kQhnMRURFiy7NsdS8EFL-8vtqEXOgECn'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('/content/AlphaPose/pretrained_models/fast_res50_256x192.pth')\n",
        "\n",
        "!wget -P ./detector/yolox/data/ https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.0/yolox_x.pth\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiJCOUjj-g-M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/AlphaPose')\n",
        "! ls\n",
        "! python3 scripts/demo_inference.py --cfg configs/coco/resnet/256x192_res50_lr1e-3_1x.yaml --checkpoint pretrained_models/fast_res50_256x192.pth --indir examples/demo/ --save_img\n",
        "# result json and rendered images are saved here:\n",
        "! ls examples/res/\n",
        "! ls examples/res/vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C17tdN7MrmyL"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3cSG5Djajea"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from alphapose.utils.config import update_config\n",
        "from alphapose.models import builder\n",
        "from alphapose.utils.transforms import get_func_heatmap_to_coord\n",
        "\n",
        "class Opt:\n",
        "    def __init__(self):\n",
        "        self.dataset = 'coco'\n",
        "        self.sp = False\n",
        "        self.save_img = False\n",
        "        self.outputpath = './results/'\n",
        "        self.pose_flow = False\n",
        "        self.vis = True\n",
        "\n",
        "class GetKeypoint:\n",
        "    NOSE = 0\n",
        "    LEFT_EYE = 1\n",
        "    RIGHT_EYE = 2\n",
        "    LEFT_EAR = 3\n",
        "    RIGHT_EAR = 4\n",
        "    LEFT_SHOULDER = 5\n",
        "    RIGHT_SHOULDER = 6\n",
        "    LEFT_ELBOW = 7\n",
        "    RIGHT_ELBOW = 8\n",
        "    LEFT_WRIST = 9\n",
        "    RIGHT_WRIST = 10\n",
        "    LEFT_HIP = 11\n",
        "    RIGHT_HIP = 12\n",
        "    LEFT_KNEE = 13\n",
        "    RIGHT_KNEE = 14\n",
        "    LEFT_ANKLE = 15\n",
        "    RIGHT_ANKLE = 16\n",
        "\n",
        "get_keypoint = GetKeypoint()\n",
        "\n",
        "def classify_pose(keypoints):\n",
        "    # 주요 신체 부위의 y좌표 설정\n",
        "    nose_y = keypoints[GetKeypoint.NOSE * 2 + 1]\n",
        "    left_shoulder_y = keypoints[GetKeypoint.LEFT_SHOULDER * 2 + 1]\n",
        "    right_shoulder_y = keypoints[GetKeypoint.RIGHT_SHOULDER * 2 + 1]\n",
        "    left_hip_y = keypoints[GetKeypoint.LEFT_HIP * 2 + 1]\n",
        "    right_hip_y = keypoints[GetKeypoint.RIGHT_HIP * 2 + 1]\n",
        "    left_knee_y = keypoints[GetKeypoint.LEFT_KNEE * 2 + 1]\n",
        "    right_knee_y = keypoints[GetKeypoint.RIGHT_KNEE * 2 + 1]\n",
        "    left_elbow_y = keypoints[GetKeypoint.LEFT_ELBOW * 2 + 1]\n",
        "    right_elbow_y = keypoints[GetKeypoint.RIGHT_ELBOW * 2 + 1]\n",
        "    left_wrist_y = keypoints[GetKeypoint.LEFT_WRIST * 2 + 1]\n",
        "    right_wrist_y = keypoints[GetKeypoint.RIGHT_WRIST * 2 + 1]\n",
        "    left_hand_y = keypoints[GetKeypoint.LEFT_WRIST * 2 + 1]\n",
        "    right_hand_y = keypoints[GetKeypoint.RIGHT_WRIST * 2 + 1]\n",
        "    left_ankle_y = keypoints[GetKeypoint.LEFT_ANKLE * 2 + 1]\n",
        "    right_ankle_y = keypoints[GetKeypoint.RIGHT_ANKLE * 2 + 1]\n",
        "\n",
        "    # 주요 신체 부위의 x좌표 설정\n",
        "    left_shoulder_x = keypoints[GetKeypoint.LEFT_SHOULDER * 2]\n",
        "    right_shoulder_x = keypoints[GetKeypoint.RIGHT_SHOULDER * 2]\n",
        "    left_hip_x = keypoints[GetKeypoint.LEFT_HIP * 2]\n",
        "    right_hip_x = keypoints[GetKeypoint.RIGHT_HIP * 2]\n",
        "    left_knee_x = keypoints[GetKeypoint.LEFT_KNEE * 2]\n",
        "    right_knee_x = keypoints[GetKeypoint.RIGHT_KNEE * 2]\n",
        "    left_elbow_x = keypoints[GetKeypoint.LEFT_ELBOW * 2]\n",
        "    right_elbow_x = keypoints[GetKeypoint.RIGHT_ELBOW * 2]\n",
        "    left_wrist_x = keypoints[GetKeypoint.LEFT_WRIST * 2]\n",
        "    right_wrist_x = keypoints[GetKeypoint.RIGHT_WRIST * 2]\n",
        "    left_hand_x = keypoints[GetKeypoint.LEFT_WRIST * 2]\n",
        "    right_hand_x = keypoints[GetKeypoint.RIGHT_WRIST * 2]\n",
        "    left_ankle_x = keypoints[GetKeypoint.LEFT_ANKLE * 2]\n",
        "    right_ankle_x = keypoints[GetKeypoint.RIGHT_ANKLE * 2]\n",
        "\n",
        "    # 주요 신체 부위 간의 차이 계산\n",
        "    shoulder_diff = abs(left_shoulder_y - right_shoulder_y)\n",
        "    hip_diff = abs(left_hip_y - right_hip_y)\n",
        "    shoulder_hip_diff = min(abs(left_shoulder_y - left_hip_y), abs(right_shoulder_y - right_hip_y))\n",
        "    hip_knee_diff = min(abs(left_hip_y - left_knee_y), abs(right_hip_y - right_knee_y))\n",
        "    elbow_wrist_diff = min(abs(left_elbow_y - left_wrist_y), abs(right_elbow_y - right_wrist_y))\n",
        "    knee_ankle_diff = min(abs(left_knee_y - left_ankle_y), abs(right_knee_y - right_ankle_y))\n",
        "    elbow_diff = abs(left_elbow_y - right_elbow_y)\n",
        "    foot_diff = abs(left_ankle_x - right_ankle_x)\n",
        "    foot_y = min(left_ankle_y, right_ankle_y)\n",
        "\n",
        "\n",
        "    if shoulder_diff < 20 and hip_diff < 20 and nose_y < min(left_shoulder_y, right_shoulder_y):\n",
        "        return 'falling'\n",
        "# 어깨와 엉덩이 차이가 크고,\n",
        "    # 어깨와 엉덩이 차이가 크고, 힙-무릎 간 차이가 적은 경우 (공격 자세)\n",
        "    elif shoulder_diff < 20 and hip_diff < 20 and shoulder_hip_diff > 50 and hip_knee_diff < 30:\n",
        "        return 'attack'\n",
        "    # 머리가 어깨보다 낮고, 어깨-엉덩이 차이가 큰 경우 (공격 또는 무방비 상태)\n",
        "    elif nose_y < min(left_shoulder_y, right_shoulder_y) and shoulder_hip_diff > 50:\n",
        "        return 'defense'\n",
        "    # 어깨-엉덩이 차이가 크고, 엉덩이-무릎 간 차이가 작으며, 무릎-발목 간 차이가 큰 경우 (공격적인 자세)\n",
        "    elif shoulder_hip_diff > 50 and hip_knee_diff < 30 and knee_ankle_diff > 50:\n",
        "        return 'aggressive'\n",
        "    # 팔꿈치-손목 간 차이가 적고, 손이 얼굴 근처에 있는 경우 (펀치 자세)\n",
        "    elif elbow_wrist_diff < 20 and min(left_hand_y, right_hand_y) < nose_y + 20:\n",
        "        return 'punch'\n",
        "    # 팔이 앞으로 뻗어 있고 손이 몸의 앞쪽에 있는 경우 (미는 동작)\n",
        "    elif elbow_wrist_diff < 20 and max(left_hand_x, right_hand_x) > max(left_shoulder_x, right_shoulder_x):\n",
        "        return 'push'\n",
        "    # 양손이 가까이 있고, 손이 상대방의 목 부근에 위치하는 경우 (멱살 잡는 동작)\n",
        "    elif abs(left_hand_x - right_hand_x) < 20 and min(left_hand_y, right_hand_y) < nose_y + 20 and min(left_hand_y, right_hand_y) > nose_y - 20:\n",
        "        return 'grab_collar'\n",
        "    # 발로 차는 동작 (Kicking)\n",
        "    elif knee_ankle_diff > 50 and foot_y < hip_diff:\n",
        "        return 'kicking'\n",
        "    # 팔꿈치로 가격하는 동작 (Elbow Strike)\n",
        "    elif elbow_diff < 20 and min(left_elbow_y, right_elbow_y) < shoulder_diff:\n",
        "        return 'elbow_strike'\n",
        "    # 밀어 넘어뜨리는 동작 (Shoving)\n",
        "    elif shoulder_hip_diff > 50 and elbow_wrist_diff < 20 and max(left_hand_x, right_hand_x) > max(left_shoulder_x, right_shoulder_x):\n",
        "        return 'shoving'\n",
        "    # 목을 조르는 동작 (Choking)\n",
        "    elif abs(left_hand_x - right_hand_x) < 20 and min(left_hand_y, right_hand_y) < nose_y + 10 and min(left_hand_y, right_hand_y) > nose_y - 10:\n",
        "        return 'choking'\n",
        "    # 발로 밟는 동작 (Stomping)\n",
        "    elif foot_diff > 50 and foot_y > min(left_ankle_y, right_ankle_y):\n",
        "        return 'stomping'\n",
        "    # 웅크리고 있는 자세 (Crouching)\n",
        "    elif shoulder_hip_diff < 50 and hip_knee_diff < 50 and min(left_knee_y, right_knee_y) < nose_y:\n",
        "        return 'crouching'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "def vis_frame(orig_img, boxes, keypoints, labels):\n",
        "    \"\"\"\n",
        "    이미지를 시각화하기 위한 함수\n",
        "    :param orig_img: 원본 이미지\n",
        "    :param boxes: 바운딩 박스 리스트\n",
        "    :param keypoints: 키포인트 리스트\n",
        "    :param labels: 포즈 라벨 리스트\n",
        "    :return: 시각화된 이미지\n",
        "    \"\"\"\n",
        "    vis_img = orig_img.copy()\n",
        "    for i, box in enumerate(boxes):\n",
        "        x1, y1, x2, y2 = box\n",
        "        cv2.rectangle(vis_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        for j in range(0, len(keypoints[i]), 2):\n",
        "            cv2.circle(vis_img, (int(keypoints[i][j]), int(keypoints[i][j + 1])), 5, (0, 255, 0), -1)\n",
        "        cv2.putText(vis_img, labels[i], (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "    return vis_img\n",
        "\n",
        "def main():\n",
        "    # YOLOv8 모델 로드\n",
        "    yolo_model = YOLO('yolov8n.pt')\n",
        "\n",
        "    image_files = []\n",
        "\n",
        "    image_directory = '/content/drive/MyDrive/논문주제/path_to_output_frames/train/Fight'\n",
        "    for root, dirs, files in os.walk(image_directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                image_files.append(os.path.join(root, file))\n",
        "\n",
        "    image_files = sorted(image_files)\n",
        "\n",
        "    output_directory = '/content/drive/MyDrive/논문주제/results4/'\n",
        "    csv_file_path = '/content/drive/MyDrive/논문주제/Final_project/Fight_pose_re_falling_keypoint.csv'\n",
        "    pretrained_model_path = '/content/drive/MyDrive/논문주제/Final_project/pretrained_models/fast_res50_256x192.pth'\n",
        "\n",
        "    if not os.path.exists(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "\n",
        "    # 설정 파일 경로\n",
        "    cfg_file = '/content/AlphaPose/configs/coco/resnet/256x192_res50_lr1e-3_1x.yaml'\n",
        "\n",
        "    # 설정 업데이트\n",
        "    cfg = update_config(cfg_file)\n",
        "    cfg['checkpoint'] = pretrained_model_path\n",
        "    cfg['outputpath'] = output_directory\n",
        "    cfg['vis'] = True\n",
        "\n",
        "    # 결과 저장 디렉토리 생성\n",
        "    os.makedirs(cfg['outputpath'], exist_ok=True)\n",
        "\n",
        "    # GPU 장치 설정\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # AlphaPose 모델 로드\n",
        "    pose_model = builder.build_sppe(cfg.MODEL, preset_cfg=cfg.DATA_PRESET)\n",
        "    print(f'Loading pose model from {cfg[\"checkpoint\"]}')\n",
        "    pose_model.load_state_dict(torch.load(cfg[\"checkpoint\"], map_location=device))\n",
        "    pose_model = torch.nn.DataParallel(pose_model).to(device)\n",
        "    pose_model.eval()\n",
        "\n",
        "    # 컬럼 이름 설정 (두 명의 사람에 대한 데이터 포함)\n",
        "        # 컬럼 이름 설정 (두 명의 사람에 대한 데이터 포함)\n",
        "    header = [\n",
        "    'image_name', 'target_person_label',\n",
        "    'target_person_nose_x', 'target_person_nose_y', 'target_person_left_eye_x', 'target_person_left_eye_y',\n",
        "    'target_person_right_eye_x', 'target_person_right_eye_y', 'target_person_left_ear_x', 'target_person_left_ear_y',\n",
        "    'target_person_right_ear_x', 'target_person_right_ear_y', 'target_person_left_shoulder_x', 'target_person_left_shoulder_y',\n",
        "    'target_person_right_shoulder_x', 'target_person_right_shoulder_y', 'target_person_left_elbow_x', 'target_person_left_elbow_y',\n",
        "    'target_person_right_elbow_x', 'target_person_right_elbow_y', 'target_person_left_wrist_x', 'target_person_left_wrist_y',\n",
        "    'target_person_right_wrist_x', 'target_person_right_wrist_y', 'target_person_left_hip_x', 'target_person_left_hip_y',\n",
        "    'target_person_right_hip_x', 'target_person_right_hip_y', 'target_person_left_knee_x', 'target_person_left_knee_y',\n",
        "    'target_person_right_knee_x', 'target_person_right_knee_y', 'target_person_left_ankle_x', 'target_person_left_ankle_y',\n",
        "    'target_person_right_ankle_x', 'target_person_right_ankle_y', 'closest_person_label',\n",
        "    'closest_person_nose_x', 'closest_person_nose_y', 'closest_person_left_eye_x', 'closest_person_left_eye_y',\n",
        "    'closest_person_right_eye_x', 'closest_person_right_eye_y', 'closest_person_left_ear_x', 'closest_person_left_ear_y',\n",
        "    'closest_person_right_ear_x', 'closest_person_right_ear_y', 'closest_person_left_shoulder_x', 'closest_person_left_shoulder_y',\n",
        "    'closest_person_right_shoulder_x', 'closest_person_right_shoulder_y', 'closest_person_left_elbow_x', 'closest_person_left_elbow_y',\n",
        "    'closest_person_right_elbow_x', 'closest_person_right_elbow_y', 'closest_person_left_wrist_x', 'closest_person_left_wrist_y',\n",
        "    'closest_person_right_wrist_x', 'closest_person_right_wrist_y', 'closest_person_left_hip_x', 'closest_person_left_hip_y',\n",
        "    'closest_person_right_hip_x', 'closest_person_right_hip_y', 'closest_person_left_knee_x', 'closest_person_left_knee_y',\n",
        "    'closest_person_right_knee_x', 'closest_person_right_knee_y', 'closest_person_left_ankle_x', 'closest_person_left_ankle_y',\n",
        "    'closest_person_right_ankle_x', 'closest_person_right_ankle_y'\n",
        "]\n",
        "    all_keypoints = []\n",
        "\n",
        "    # 이전에 저장된 체크포인트 확인\n",
        "    if os.path.exists(csv_file_path):\n",
        "        df_existing = pd.read_csv(csv_file_path)\n",
        "        processed_images = set(df_existing['image_name'].tolist())\n",
        "    else:\n",
        "        df_existing = pd.DataFrame(columns=header)\n",
        "        processed_images = set()\n",
        "\n",
        "    # 모든 이미지 파일 처리\n",
        "    for idx, image_file in enumerate(image_files):\n",
        "        if image_file in processed_images:\n",
        "            continue  # 이미 처리된 이미지라면 건너뛰기\n",
        "\n",
        "        input_image_path = os.path.join(image_directory, image_file)\n",
        "        input_image = cv2.imread(input_image_path)\n",
        "        if input_image is None:\n",
        "            print(f'Error: Image {input_image_path} not found or could not be read.')\n",
        "            continue\n",
        "\n",
        "        # YOLO 모델 실행 후 사람 객체 필터링\n",
        "        results = yolo_model.predict(input_image, save=False, classes=[0])\n",
        "        human_detections = [d for d in results[0].boxes.data.cpu().numpy() if int(d[-1]) == 0]\n",
        "\n",
        "        if not human_detections:\n",
        "            print(f'No human detections in image {input_image_path}')\n",
        "            continue\n",
        "\n",
        "        inps = []\n",
        "        boxes = []\n",
        "        for detection in human_detections:\n",
        "            x1, y1, x2, y2 = map(int, detection[:4])\n",
        "            boxes.append([x1, y1, x2, y2])\n",
        "            inp = cv2.resize(input_image[y1:y2, x1:x2], (cfg.DATA_PRESET.IMAGE_SIZE[0], cfg.DATA_PRESET.IMAGE_SIZE[1]))\n",
        "            inps.append(inp)\n",
        "\n",
        "        inps = torch.stack([torch.from_numpy(np.array(inp)).permute(2, 0, 1).float() for inp in inps]).to(device)\n",
        "\n",
        "        # AlphaPose 모델로 스켈레톤 추출\n",
        "        with torch.no_grad():\n",
        "            hm = pose_model(inps)\n",
        "\n",
        "        # 스켈레톤 결과를 처리하여 CSV 파일로 저장\n",
        "        keypoints = []\n",
        "        labels = []\n",
        "        for i, box in enumerate(boxes):\n",
        "            preds, maxvals = get_func_heatmap_to_coord(cfg)(hm[i], box)\n",
        "            keypoints_flatten = preds.flatten().tolist()\n",
        "            keypoints.append(keypoints_flatten)\n",
        "            labels.append(classify_pose(keypoints_flatten))\n",
        "\n",
        "        # 100번에 한 번씩 이미지를 저장\n",
        "        if idx % 100 == 0:\n",
        "            result_img = vis_frame(input_image, boxes, keypoints, labels)\n",
        "            output_path = os.path.join(output_directory, f'result_{os.path.basename(input_image_path)}')\n",
        "            cv2.imwrite(output_path, result_img)\n",
        "            print(f'Results saved at {output_path}')\n",
        "\n",
        "\n",
        "        # 두 명의 사람에 대한 키포인트 저장 및 조건 처리\n",
        "        if len(keypoints) >= 2:\n",
        "            # 넘어진 사람이 있는지 확인\n",
        "            falling_indices = [i for i, label in enumerate(labels) if label == 'falling']\n",
        "\n",
        "            if falling_indices:\n",
        "                # 넘어진 사람의 인덱스와 키포인트\n",
        "                fallen_index = falling_indices[0]\n",
        "                fallen_keypoints = keypoints[fallen_index]\n",
        "\n",
        "                # 넘어진 사람과 가장 가까운 사람 찾기\n",
        "                min_distance = float('inf')\n",
        "                closest_person_index = None\n",
        "\n",
        "                for i, kp in enumerate(keypoints):\n",
        "                    if i != fallen_index:\n",
        "                        # 두 사람 간의 유클리드 거리 계산 (코어 위치 기준: 엉덩이 좌표 사용)\n",
        "                        distance = np.linalg.norm(np.array(fallen_keypoints[GetKeypoint.LEFT_HIP*2:GetKeypoint.LEFT_HIP*2+2]) -\n",
        "                                                  np.array(kp[GetKeypoint.LEFT_HIP*2:GetKeypoint.LEFT_HIP*2+2]))\n",
        "                        if distance < min_distance:\n",
        "                            min_distance = distance\n",
        "                            closest_person_index = i\n",
        "\n",
        "                # 넘어진 사람과 가장 가까운 사람의 키포인트 저장\n",
        "                keypoints_data = [image_file, labels[fallen_index]] + keypoints[fallen_index] + [labels[closest_person_index]] + keypoints[closest_person_index]\n",
        "\n",
        "            else:\n",
        "                # 웅크리고 있는 사람이 있는지 확인\n",
        "                crouching_indices = [i for i, label in enumerate(labels) if label == 'crouching']\n",
        "\n",
        "                if crouching_indices:\n",
        "                    # 웅크린 사람의 인덱스와 키포인트\n",
        "                    crouching_index = crouching_indices[0]\n",
        "                    crouching_keypoints = keypoints[crouching_index]\n",
        "\n",
        "                    # 웅크린 사람과 가장 가까운 사람 찾기\n",
        "                    min_distance = float('inf')\n",
        "                    closest_person_index = None\n",
        "\n",
        "                    for i, kp in enumerate(keypoints):\n",
        "                        if i != crouching_index:\n",
        "                            # 두 사람 간의 유클리드 거리 계산 (코어 위치 기준: 엉덩이 좌표 사용)\n",
        "                            distance = np.linalg.norm(np.array(crouching_keypoints[GetKeypoint.LEFT_HIP*2:GetKeypoint.LEFT_HIP*2+2]) -\n",
        "                                                      np.array(kp[GetKeypoint.LEFT_HIP*2:GetKeypoint.LEFT_HIP*2+2]))\n",
        "                            if distance < min_distance:\n",
        "                                min_distance = distance\n",
        "                                closest_person_index = i\n",
        "\n",
        "                    # 웅크린 사람과 가장 가까운 사람의 키포인트 저장\n",
        "                    keypoints_data = [image_file, labels[crouching_index]] + keypoints[crouching_index] + [labels[closest_person_index]] + keypoints[closest_person_index]\n",
        "\n",
        "                else:\n",
        "                    # 넘어진 사람이나 웅크린 사람이 없을 경우, 가장 가까운 두 사람을 선택\n",
        "                    min_distance = float('inf')\n",
        "                    person1_index, person2_index = None, None\n",
        "\n",
        "                    for i in range(len(keypoints)):\n",
        "                        for j in range(i + 1, len(keypoints)):\n",
        "                            # 두 사람 간의 유클리드 거리 계산 (코어 위치 기준: 엉덩이 좌표 사용)\n",
        "                            distance = np.linalg.norm(np.array(keypoints[i][GetKeypoint.LEFT_HIP*2:GetKeypoint.LEFT_HIP*2+2]) -\n",
        "                                                      np.array(keypoints[j][GetKeypoint.LEFT_HIP*2:GetKeypoint.LEFT_HIP*2+2]))\n",
        "                            if distance < min_distance:\n",
        "                                min_distance = distance\n",
        "                                person1_index, person2_index = i, j\n",
        "\n",
        "                    # 가장 가까운 두 사람의 키포인트 저장\n",
        "                    keypoints_data = [image_file, labels[person1_index]] + keypoints[person1_index] + [labels[person2_index]] + keypoints[person2_index]\n",
        "\n",
        "        elif len(keypoints) == 1:\n",
        "            # 한 명만 있는 경우, 그 사람의 데이터와 기본값 저장\n",
        "            keypoints_data = [image_file, labels[0]] + keypoints[0] + [''] + [0] * 34\n",
        "\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # 주기적으로 CSV 파일 저장\n",
        "        if idx % 10 == 0:  # 매번 이미지마다 CSV 파일에 저장\n",
        "            df_temp = pd.DataFrame(all_keypoints, columns=header)\n",
        "            df_temp.to_csv(csv_file_path, mode='a', header=not os.path.exists(csv_file_path), index=False)\n",
        "            print(f'Progress saved at {csv_file_path}')\n",
        "            all_keypoints = []  # 임시 리스트 초기화\n",
        "\n",
        "        # 여기서 keypoints_data를 all_keypoints 리스트에 추가\n",
        "        all_keypoints.append(keypoints_data)\n",
        "\n",
        "\n",
        "    if all_keypoints:\n",
        "        df_temp = pd.DataFrame(all_keypoints, columns=header)\n",
        "        # 동일한 이미지 이름의 데이터가 이미 존재하는지 확인\n",
        "        df_existing = pd.read_csv(csv_file_path)\n",
        "        df_combined = pd.concat([df_existing, df_temp], ignore_index=True)\n",
        "\n",
        "        # 중복된 이미지 이름 제거 (최근의 데이터만 유지)\n",
        "        df_combined.drop_duplicates(subset=['image_name'], keep='last', inplace=True)\n",
        "\n",
        "        df_combined.to_csv(csv_file_path, index=False)\n",
        "        print(f'Progress saved at {csv_file_path}')\n",
        "        all_keypoints = []  # 임시 리스트 초기화\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G4n8BkvGzj7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTOKkWr4bIbW"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvxySEI5AzC9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R19psejwxn_w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/논문주제/Final_project/train_pose2_keypoint.csv')\n",
        "\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWxNJGQz33ud"
      },
      "outputs": [],
      "source": [
        "row_lengths = df.apply(len, axis=1)\n",
        "\n",
        "# 첫 5개의 행의 길이 출력\n",
        "print(\"첫 5개 행의 길이:\", row_lengths.head())\n",
        "\n",
        "# 전체 데이터에 대한 통계(최소, 최대, 평균 길이) 확인\n",
        "print(\"최소 길이:\", row_lengths.min())\n",
        "print(\"최대 길이:\", row_lengths.max())\n",
        "print(\"평균 길이:\", row_lengths.mean())\n",
        "\n",
        "# 전체 데이터 길이 확인\n",
        "print(\"전체 데이터 길이:\", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('/content/drive/MyDrive/논문주제/Final_project/NonFight_pose2_keypoint.csv')\n",
        "\n",
        "df2.head(10)"
      ],
      "metadata": {
        "id": "GIeX7mxq66f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.columns"
      ],
      "metadata": {
        "id": "TNtnurXG8cqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row_lengths2 = df2.apply(len, axis=1)\n",
        "\n",
        "# 첫 5개의 행의 길이 출력\n",
        "print(\"첫 5개 행의 길이:\", row_lengths2.head())\n",
        "\n",
        "# 전체 데이터에 대한 통계(최소, 최대, 평균 길이) 확인\n",
        "print(\"최소 길이:\", row_lengths2.min())\n",
        "print(\"최대 길이:\", row_lengths2.max())\n",
        "print(\"평균 길이:\", row_lengths2.mean())\n",
        "\n",
        "# 전체 데이터 길이 확인\n",
        "print(\"전체 데이터 길이:\", len(df2))"
      ],
      "metadata": {
        "id": "SW74vseG7Iu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['closest_person_label'] = df2['closest_person_label'].apply(lambda x: 'not' + x if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "rej-fxSC7QlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2"
      ],
      "metadata": {
        "id": "pnga3jT68DEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.to_csv('/content/drive/MyDrive/논문주제/Final_project/NonFight_pose2_keypoint.csv', index=False)"
      ],
      "metadata": {
        "id": "0baEdoaH8k0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iMARrZNi0Kn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 각 파일 경로를 리스트로 저장\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/논문주제/Final_project/train_pose2_keypoint.csv',\n",
        "    '/content/drive/MyDrive/논문주제/Final_project/NonFight_pose2_keypoint.csv'\n",
        "]\n",
        "\n",
        "# 빈 데이터프레임 생성\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# 각 파일을 불러와서 합치기\n",
        "for file_path in file_paths:\n",
        "    # CSV 파일 불러오기\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # 모든 값이 채워진 행 선택\n",
        "    complete_rows = df.dropna()\n",
        "\n",
        "    # 30,000개의 행을 선택 (부족할 경우 결측치가 있는 행 추가)\n",
        "    if len(complete_rows) >= 30000:\n",
        "        selected_df = complete_rows.iloc[:30000]\n",
        "    else:\n",
        "        # 결측치가 있는 행 선택\n",
        "        incomplete_rows = df[df.isna().any(axis=1)]\n",
        "        # 부족한 부분을 결측치가 있는 행으로 채움\n",
        "        remaining_rows = 30000 - len(complete_rows)\n",
        "        selected_df = pd.concat([complete_rows, incomplete_rows.iloc[:remaining_rows]], ignore_index=True)\n",
        "\n",
        "    # 결과를 combined_df에 추가\n",
        "    combined_df = pd.concat([combined_df, selected_df], ignore_index=True)\n",
        "\n",
        "# 결과 출력\n",
        "print(combined_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QP0YNUNjmyUo"
      },
      "outputs": [],
      "source": [
        "# 얼굴 관련 키포인트 제거\n",
        "columns_to_remove = [\n",
        "    'target_person_nose_x', 'target_person_nose_y', 'target_person_left_eye_x', 'target_person_left_eye_y',\n",
        "    'target_person_right_eye_x', 'target_person_right_eye_y', 'target_person_left_ear_x', 'target_person_left_ear_y',\n",
        "    'target_person_right_ear_x', 'target_person_right_ear_y',\n",
        "    'closest_person_nose_x', 'closest_person_nose_y', 'closest_person_left_eye_x', 'closest_person_left_eye_y',\n",
        "    'closest_person_right_eye_x', 'closest_person_right_eye_y', 'closest_person_left_ear_x', 'closest_person_left_ear_y',\n",
        "    'closest_person_right_ear_x', 'closest_person_right_ear_y'\n",
        "]\n",
        "\n",
        "\n",
        "# 데이터프레임에서 해당 컬럼 제거\n",
        "combined_df = combined_df.drop(columns=columns_to_remove)\n",
        "\n",
        "# 결과 확인\n",
        "print(combined_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4llypaim2v5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 얼굴 관련 키포인트 제거 후 남은 keypoints 리스트 정의\n",
        "keypoints = [\n",
        "    'target_person_left_shoulder_x', 'target_person_left_shoulder_y',\n",
        "    'target_person_right_shoulder_x', 'target_person_right_shoulder_y',\n",
        "    'target_person_left_elbow_x', 'target_person_left_elbow_y',\n",
        "    'target_person_right_elbow_x', 'target_person_right_elbow_y',\n",
        "    'target_person_left_wrist_x', 'target_person_left_wrist_y',\n",
        "    'target_person_right_wrist_x', 'target_person_right_wrist_y',\n",
        "    'target_person_left_hip_x', 'target_person_left_hip_y',\n",
        "    'target_person_right_hip_x', 'target_person_right_hip_y',\n",
        "    'target_person_left_knee_x', 'target_person_left_knee_y',\n",
        "    'target_person_right_knee_x', 'target_person_right_knee_y',\n",
        "    'target_person_left_ankle_x', 'target_person_left_ankle_y',\n",
        "    'target_person_right_ankle_x', 'target_person_right_ankle_y',\n",
        "    'closest_person_left_shoulder_x', 'closest_person_left_shoulder_y',\n",
        "    'closest_person_right_shoulder_x', 'closest_person_right_shoulder_y',\n",
        "    'closest_person_left_elbow_x', 'closest_person_left_elbow_y',\n",
        "    'closest_person_right_elbow_x', 'closest_person_right_elbow_y',\n",
        "    'closest_person_left_wrist_x', 'closest_person_left_wrist_y',\n",
        "    'closest_person_right_wrist_x', 'closest_person_right_wrist_y',\n",
        "    'closest_person_left_hip_x', 'closest_person_left_hip_y',\n",
        "    'closest_person_right_hip_x', 'closest_person_right_hip_y',\n",
        "    'closest_person_left_knee_x', 'closest_person_left_knee_y',\n",
        "    'closest_person_right_knee_x', 'closest_person_right_knee_y',\n",
        "    'closest_person_left_ankle_x', 'closest_person_left_ankle_y',\n",
        "    'closest_person_right_ankle_x', 'closest_person_right_ankle_y'\n",
        "]\n",
        "\n",
        "# 시퀀스 길이 설정\n",
        "n_timesteps = 20\n",
        "\n",
        "# 레이블을 원-핫 인코딩하는 함수\n",
        "def one_hot_encode(labels):\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded = np.array([label_map[label] for label in labels])\n",
        "    return np.eye(len(unique_labels))[encoded]\n",
        "\n",
        "# 시퀀스 데이터를 생성하는 함수\n",
        "def create_sequences(data, seq_length):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data) - seq_length):\n",
        "        seq = data.iloc[i:i + seq_length]\n",
        "        label = data.iloc[i + seq_length - 1]['target_person_label']\n",
        "\n",
        "        # 필요 없는 컬럼을 제외하고 시퀀스 저장\n",
        "        X.append(seq[keypoints].values)\n",
        "        y.append(label)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 30,000개의 로우만 사용하도록 데이터프레임을 슬라이스\n",
        "subset_df = combined_df.iloc[:60000]\n",
        "\n",
        "# 시퀀스 생성\n",
        "X, y = create_sequences(subset_df, n_timesteps)\n",
        "\n",
        "# 레이블 원-핫 인코딩\n",
        "y = one_hot_encode(y)\n",
        "\n",
        "# 입력 데이터와 출력 데이터의 shape 확인\n",
        "print(\"X의 shape:\", X.shape)\n",
        "print(\"y의 shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ndyhGnRafiO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 학습 데이터와 테스트 데이터 분리\n",
        "# train test split\n",
        "# 학습 데이터와 테스트 데이터 분리 (stratify 옵션 제거)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, shuffle=True, random_state=7461\n",
        ")\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xweJLAHh9XR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_dataset(X, y, batch_size=32, shuffle=True):\n",
        "    # 데이터셋 객체 생성\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "\n",
        "    # 데이터셋 섞기\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(X))\n",
        "\n",
        "    # 배치 크기로 분할\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# 배치 사이즈 설정\n",
        "batch_size = 64\n",
        "\n",
        "# 데이터 세트 생성\n",
        "dataset_train = create_dataset(X_train, y_train, batch_size)\n",
        "dataset_test = create_dataset(X_test, y_test, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uw5497dFhboA"
      },
      "outputs": [],
      "source": [
        "len(dataset_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgymogdFhsDK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, LayerNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# 모델 파라미터 설정\n",
        "input_shape = (X.shape[1], X.shape[2])  # 입력 시퀀스의 형태 (시퀀스 길이, 특성 수)\n",
        "num_classes = y.shape[1]  # 출력 클래스 수\n",
        "\n",
        "# Early stopping 설정\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0.001,\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# 모델 생성 함수 정의\n",
        "def build_model(lstm_layers, input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM layers\n",
        "    units = [128, 64, 32]  # 각 LSTM 레이어의 유닛 수\n",
        "    for i in range(lstm_layers):\n",
        "        return_sequences = i < lstm_layers - 1  # 마지막 LSTM layer는 return_sequences=False\n",
        "        model.add(LSTM(units[i], return_sequences=return_sequences, activation='tanh', input_shape=input_shape if i == 0 else None))\n",
        "        model.add(LayerNormalization())\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "    # Dense layers\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# LSTM layer를 1, 2, 3개 사용하는 모델을 각각 생성 및 훈련\n",
        "models = {}\n",
        "for lstm_layers in [1, 2, 3]:\n",
        "    print(f\"Training model with {lstm_layers} LSTM layer(s)...\")\n",
        "    model = build_model(lstm_layers, input_shape, num_classes)\n",
        "    model.summary()\n",
        "    history = model.fit(dataset_train, epochs=100, callbacks=[early_stopping], validation_data=dataset_test, verbose=0)\n",
        "\n",
        "    # 검증 데이터에 대한 예측 수행\n",
        "    y_pred = model.predict(dataset_test)\n",
        "\n",
        "    # 예측된 확률을 클래스 레이블로 변환\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)  # 실제 클래스 레이블\n",
        "\n",
        "    # 정밀도, 재현율, F1 점수 계산\n",
        "    precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "\n",
        "    # 모델 결과 저장\n",
        "    models[f'{lstm_layers}_LSTM'] = {\n",
        "        'accuracy': history.history['val_accuracy'][-1],\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "    print(f\"Model with {lstm_layers} LSTM layer(s):\")\n",
        "    print(f\"Accuracy: {history.history['val_accuracy'][-1]}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkO2HmAlZ9C8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 모델의 성능 지표를 시각화하기 위한 함수 정의\n",
        "def plot_model_performance(models):\n",
        "    lstm_layers = ['1_LSTM', '2_LSTM', '3_LSTM']  # LSTM 레이어 수에 따른 키 설정\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']  # 시각화할 성능 지표\n",
        "\n",
        "    for metric in metrics:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        values = [models[layer][metric] for layer in lstm_layers]\n",
        "        plt.plot(lstm_layers, values, marker='o', linestyle='-', label=metric)\n",
        "        plt.title(f'모델 성능 - {metric.capitalize()}')\n",
        "        plt.xlabel('LSTM 레이어 수')\n",
        "        plt.ylabel(metric.capitalize())\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "# 모델의 성능 지표를 시각화\n",
        "plot_model_performance(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSPvJKugaM9M"
      },
      "outputs": [],
      "source": [
        "# 모델을 HDF5 파일 형식으로 저장\n",
        "model.save('/content/drive/MyDrive/논문주제/Final_project/not_my_model2_lstm.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mhn_5IOmhv1k"
      },
      "outputs": [],
      "source": [
        "# 훈련 과정 시각화 (선택적)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'], label='Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.plot(history.history['loss'], label='Loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.title('lstm3')\n",
        "plt.ylabel('Value')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP9o6dy9dvdb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBPjOByodxIf"
      },
      "source": [
        "영상테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jbMJ1mpiHA6"
      },
      "outputs": [],
      "source": [
        "!wget -q -O /content/DejaVuSans-Bold.ttf https://github.com/dejavu-fonts/dejavu-fonts/raw/master/ttf/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5CUKZZzLeBLM"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab.patches import cv2_imshow  # 코랩 환경에서 이미지 표시를 위한 모듈\n",
        "import os\n",
        "\n",
        "# 영상 파일 위치 지정\n",
        "video_file = '/content/drive/MyDrive/논문주제/Final_project/Dataset/학교폭력 영상/F_176_1_0_0_0.mp4'\n",
        "\n",
        "# YOLO 모델 정의 (학습 당시에는 v8x로 진행)\n",
        "yolo_model = YOLO('yolov8n.pt')\n",
        "\n",
        "# LSTM 모델 정의\n",
        "lstm_model = load_model('/content/drive/MyDrive/논문주제/Final_project/my_model_lstm2.h5')\n",
        "\n",
        "# 시퀀스 정의\n",
        "sequences = []\n",
        "\n",
        "# 시퀀스 길이\n",
        "sequence_length = 8\n",
        "\n",
        "# 동작 라벨 정의\n",
        "actions = [\n",
        "    \"falling\", \"attack\", \"defense\", \"aggressive\", \"punch\", \"push\",\n",
        "    \"grab_collar\", \"kicking\", \"elbow_strike\", \"shoving\",\n",
        "    \"choking\", \"stomping\", \"crouching\", \"neutral\"\n",
        "]\n",
        "\n",
        "# 위험한 동작 라벨 정의\n",
        "dangerous_actions = [\n",
        "    \"falling\", \"attack\", \"aggressive\", \"punch\", \"grab_collar\",\n",
        "    \"kicking\", \"elbow_strike\", \"shoving\", \"choking\", \"stomping\"\n",
        "]\n",
        "\n",
        "# 이전에 그려진 박스의 좌표를 저장할 변수\n",
        "prev_box = None\n",
        "\n",
        "# 동영상 파일로부터 비디오 캡처 객체 생성\n",
        "cap = cv2.VideoCapture(video_file)\n",
        "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
        "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
        "\n",
        "\n",
        "def extract_keypoint(keypoints):\n",
        "    # keypoints: [x, y, confidence, ...]\n",
        "    return keypoints[:, :2].flatten().tolist()  # x, y 좌표만 사용\n",
        "\n",
        "def find_closest_pairs(boxes, labels, keypoints):\n",
        "    \"\"\"\n",
        "    주어진 조건에 따라 두 사람 간의 가장 적합한 바운딩 박스를 반환합니다.\n",
        "    \"\"\"\n",
        "    min_distance = float('inf')\n",
        "    closest_pair = None\n",
        "\n",
        "    # 쓰러진 사람이 있는지 확인\n",
        "    falling_indices = [i for i, label in enumerate(labels) if label == 'falling']\n",
        "    if falling_indices:\n",
        "        fallen_index = falling_indices[0]\n",
        "        fallen_keypoints = keypoints[fallen_index]\n",
        "\n",
        "        for i, kp in enumerate(keypoints):\n",
        "            if i != fallen_index:\n",
        "                distance = np.linalg.norm(np.array(fallen_keypoints[GetKeypoint.LEFT_HIP*2:GetKeypoint.LEFT_HIP*2+2]) -\n",
        "                                          np.array(kp[GetKeypoint.LEFT_HIP*2:GetKeypoint.LEFT_HIP*2+2]))\n",
        "                if distance < min_distance:\n",
        "                    min_distance = distance\n",
        "                    closest_pair = (fallen_index, i)\n",
        "\n",
        "    # 웅크리고 있는 사람이 있는지 확인\n",
        "    elif not closest_pair:\n",
        "        crouching_indices = [i for i, label in enumerate(labels) if label == 'crouching']\n",
        "        if crouching_indices:\n",
        "            crouching_index = crouching_indices[0]\n",
        "            crouching_keypoints = keypoints[crouching_index]\n",
        "\n",
        "            for i, kp in enumerate(keypoints):\n",
        "                if i != crouching_index:\n",
        "                    distance = np.linalg.norm(np.array(crouching_keypoints[GetKeypoint.LEFT_HIP*2:GetKeypoint.LEFT_HIP*2+2]) -\n",
        "                                              np.array(kp[GetKeypoint.LEFT_HIP*2:GetKeypoint.LEFT_HIP*2+2]))\n",
        "                    if distance < min_distance:\n",
        "                        min_distance = distance\n",
        "                        closest_pair = (crouching_index, i)\n",
        "\n",
        "    # 두 사람이 있는 경우, 가장 가까운 두 사람을 선택\n",
        "    if not closest_pair:\n",
        "        for i in range(len(boxes)):\n",
        "            for j in range(i + 1, len(boxes)):\n",
        "                center_i = [(boxes[i][0] + boxes[i][2]) / 2, (boxes[i][1] + boxes[i][3]) / 2]\n",
        "                center_j = [(boxes[j][0] + boxes[j][2]) / 2, (boxes[j][1] + boxes[j][3]) / 2]\n",
        "                distance = np.linalg.norm(np.array(center_i) - np.array(center_j))\n",
        "\n",
        "                if distance < min_distance:\n",
        "                    min_distance = distance\n",
        "                    closest_pair = (i, j)\n",
        "\n",
        "    return [boxes[closest_pair[0]], boxes[closest_pair[1]]]\n",
        "\n",
        "def create_largest_box(box1, box2):\n",
        "    \"\"\"\n",
        "    두 사람의 바운딩 박스를 감싸는 가장 큰 바운딩 박스를 생성합니다.\n",
        "    \"\"\"\n",
        "    x1 = min(box1[0], box2[0])\n",
        "    y1 = min(box1[1], box2[1])\n",
        "    x2 = max(box1[2], box2[2])\n",
        "    y2 = max(box1[3], box2[3])\n",
        "    return [x1, y1, x2, y2]\n",
        "\n",
        "def calculate_font_center(box):\n",
        "    \"\"\"\n",
        "    바운딩 박스의 중앙 위치를 계산하여 텍스트를 표시할 위치를 반환합니다.\n",
        "    \"\"\"\n",
        "    center_x = (box[0] + box[2]) / 2\n",
        "    center_y = (box[1] + box[3]) / 2\n",
        "    return center_x, center_y\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if not success:\n",
        "        break\n",
        "\n",
        "    results = yolo_model(frame, save=False, classes=[0])\n",
        "\n",
        "    if not results:\n",
        "        continue\n",
        "\n",
        "    # 키포인트가 있는지 확인하고 처리\n",
        "    if results[0].keypoints is not None:\n",
        "        results_keypoint = results[0].keypoints.xyn.cpu().numpy()\n",
        "        labels = [classify_pose(extract_keypoint(kp)) for kp in results_keypoint]\n",
        "\n",
        "        if len(results_keypoint) == 1:\n",
        "            keypoint_list = extract_keypoint(results_keypoint[0])\n",
        "            keypoint_list.extend([0] * 24)\n",
        "        elif len(results_keypoint) >= 2:\n",
        "            xy_list = results[0].boxes.xyxy.cpu().numpy().tolist()\n",
        "            closest_boxes = find_closest_pairs(xy_list, labels, results_keypoint)\n",
        "            keypoint_list1 = extract_keypoint(results_keypoint[0])\n",
        "            keypoint_list2 = extract_keypoint(results_keypoint[1])\n",
        "            keypoint_list = keypoint_list1 + keypoint_list2\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        sequences.append(keypoint_list)\n",
        "\n",
        "    if len(sequences) == sequence_length:\n",
        "        sequences_array = np.array(sequences).reshape(1, sequence_length, -1)\n",
        "        predictions = lstm_model.predict(sequences_array)\n",
        "\n",
        "        action_index = np.argmax(predictions)\n",
        "        action_name = actions[action_index]\n",
        "        max_value = np.max(predictions)\n",
        "\n",
        "        if action_name in dangerous_actions and max_value > 0.9:\n",
        "            box_color = (0, 0, 255)\n",
        "            last_dangerous_action_detected = action_name\n",
        "        elif max_value > 0.5:\n",
        "            box_color = (0, 128, 255)\n",
        "            last_dangerous_action_detected = None\n",
        "        else:\n",
        "            box_color = (0, 255, 0)\n",
        "            last_dangerous_action_detected = None\n",
        "\n",
        "        largest_box = create_largest_box(*closest_boxes)\n",
        "\n",
        "        prev_box = largest_box\n",
        "        sequences.clear()\n",
        "\n",
        "    if prev_box is not None:\n",
        "        pil_im = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        draw = ImageDraw.Draw(pil_im)\n",
        "\n",
        "        draw.rectangle(((prev_box[0], prev_box[1]), (prev_box[2], prev_box[3])), outline=box_color, width=5)\n",
        "        draw.text((prev_box[0], prev_box[1] - 40), action_name, font=font, fill=box_color)\n",
        "\n",
        "        if last_dangerous_action_detected:\n",
        "            danger_text = f\"Danger! {last_dangerous_action_detected} detected!\"\n",
        "            draw.text((50, 100), danger_text, font=font, fill=(255, 0, 0))\n",
        "\n",
        "        frame = cv2.cvtColor(np.array(pil_im), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    cv2_imshow(frame)  # 코랩 환경에서는 cv2_imshow를 사용\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YupojcfklYn1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}